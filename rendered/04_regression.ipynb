{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*updated 18 Jan 2025, Julian Mak (whatever with copyright, do what you want with this)\n",
    "\n",
    "### As part of material for OCES 3301 \"Data Analysis in Ocean Sciences\" delivered at HKUST\n",
    "\n",
    "For the latest version of the material, go to the public facing [GitHub](https://github.com/julianmak/academic-notes/tree/master/OCES3301_data_analysis_ocean) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some deafult packages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "# 04: (multi-)linear regression and others\n",
    "\n",
    "Here we are going to touch on a selection of useful tools for dealing with higher dimensional data that are somewhat commonly encountered. For this bit I found it slightly unsatisfactory using the artificial data I attempted to create (I tried a bit but not that hard...), so we are actually going to use the Penguin data in `penguins.csv` (this version is one I manually cleaned up and is modified slightly from the original)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "# a) multi-linear regression\n",
    "\n",
    "Recall linear regression as visited last time is that, given some input data $x_i$ and output data $y_i$, you find the $a$ and $b$ values in $y = f(x) = ax + b$ such that some measure of mismatch is minimised; generally we look for the $\\ell_2$ minimiser, i.e. values of $a$ and $b$ where the mean squared error $\\sum (f(x_i) - y_i)^2$ is minimised. Beyond linear regression, we can fit quadratics, cubics etc., with increasing the number of coefficients.\n",
    "\n",
    "There is of course no stopping you increasing the number of inputs instead, so you could have\n",
    "\n",
    "\\begin{equation*}\n",
    "    y = f(x, w, v, u, \\ldots) = ax + bw + cv + du + \\ldots + {\\rm constant} = C + \\sum_{j=1}^N a_j x_j,\n",
    "\\end{equation*}\n",
    "\n",
    "where you are looking for the coefficients $a_j$ and constant $C$ that minimises the mismatch $(y_i - f(x_{i,j})^2$ for given variable $x_j$ and sample value of that type $x_{i,j}$. This would be **multi-linear regression**, \"multi* because the number of variables is more than 1, and \"linear\" because everything is kept at the linear level. A concrete example could be\n",
    "\n",
    "\\begin{equation*}\n",
    "    {\\rm cursedness} = a \\times {\\rm size} + b \\times {\\rm distortion} + c \\times {\\rm Eldritch\\ attribute} + d \\times {\\rm colour} + \\ldots\n",
    "\\end{equation*}\n",
    "\n",
    "<img src=\"https://i.kym-cdn.com/photos/images/newsfeed/001/657/476/0da\" width=\"400\" alt='Eldritch horror'>\n",
    "\n",
    "Note here that there is a **single** output (we are not going to touch on **multi-variate regression** here). One could in principle add more complexity (either as number of input variables, or functional dependence), but you should bear in mind that this can lead to over-fitting; see *03_regression* for how this could cause problems.\n",
    "\n",
    "> NOTE: ***again, I cannot stress this enough, regression by default gives you correlations, and correlations do not ncessarily imply causations***. Anyone who over-claims in this course will incur the wrath of cathulu and have credit deducted from their assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step -1: have a look at the data first\n",
    "\n",
    "Lets do this with the penguin data. Again, we should probably have a look the data first (https://raw.githubusercontent.com/julianmak/OCES3301_data_analysis/refs/heads/main/penguins.csv). Below is just the code to read the Penguin data in Pandas, and is the same one as *02_reading_data*. You could always also look up the raw data file itself too, since this is a text file that you could just open with your favourite text editor.\n",
    "\n",
    "<img src=\"https://www.boredpanda.com/blog/wp-content/uploads/2020/08/cats-standing-like-penguins-fb-png__700.jpg\" width=\"500\" alt='cursed penguins'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"penguins.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES3301_data_analysis/refs/heads/main/penguins.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "print(\"===============================\")\n",
    "print(\"sample of data\")\n",
    "print(\"===============================\")\n",
    "print(df.sample(10))\n",
    "print()\n",
    "print(\"===============================\")\n",
    "print(\"some basic statistics from data\")\n",
    "print(\"===============================\")\n",
    "print(df.describe())\n",
    "\n",
    "# slice out one of the species and have a look at the basic statistics\n",
    "adelie_data = df.loc[df[\"species\"] == \"Adelie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelie_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a bit more content, here are some **pair plots** of the variables with the penguins data that I did by hand. Have a see what you think these are doing.\n",
    "\n",
    "> NOTE: You can do the below in one line and with a much nicer result in the `seaborn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "count = 0\n",
    "for i in range(1, len(adelie_data.columns[0::])):\n",
    "    for j in range(1, len(adelie_data.columns[0::])):\n",
    "        count += 1\n",
    "        ax = plt.subplot(4, 4, count)\n",
    "        ax.scatter(adelie_data[adelie_data.columns[i]], adelie_data[adelie_data.columns[j]])\n",
    "        ax.grid(lw=0.5, zorder=0)\n",
    "        if j == 1:\n",
    "            ax.set_ylabel(adelie_data.columns[i])\n",
    "        if i == len(adelie_data.columns[0::])-1:\n",
    "            ax.set_xlabel(adelie_data.columns[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> Describe the code above and explain what it is doing.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Why are there 16 plots? Also, of the 16 plots, only 6 of these are actually conveying new information, which ones, and why?\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> For the 6 plots that actually convey new information, do a linear regression on these pairs, and put line of best fit as well as write in the $r$-values on the plot (do it however you wish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is some Pandas command to compute some relevant correlation coefficients. We will do a few more of these in *05/06_statistical_tests*.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Check the Pandas outputs against the ones you did by hand just and make sure they are the same (or, if they are not the same, chase up on why they are not the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===============================\")\n",
    "print(\"Pearson correlation\")\n",
    "print(\"===============================\")\n",
    "print(df.corr(method=\"pearson\", numeric_only = True))  # warning if \"numeric_only\" not specified\n",
    "print()\n",
    "print(\"===============================\")\n",
    "print(\"Spearman correlation\")\n",
    "print(\"===============================\")\n",
    "print(df.corr(method=\"spearman\", numeric_only = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 0: look at the data but the data of interest\n",
    "\n",
    "Suppose from step -1 above you decided that ***body mass*** is the variable you want to predict, and you think you can predict the bill length with data related to the ***bill***. We are only going to just do this for the *Adelie* data for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var = adelie_data[['bill_length_mm', 'bill_depth_mm']].values\n",
    "output_var = adelie_data['body_mass_g'].values\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax.scatter(input_var[:, 0], output_var)\n",
    "ax.set_xlabel(r\"input 1\"); ax.set_ylabel(r\"output\"); ax.grid(lw=0.5, zorder=0)\n",
    "\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "ax.scatter(input_var[:, 1], output_var)\n",
    "ax.set_xlabel(r\"input 2\"); ax.set_ylabel(r\"output\"); ax.grid(lw=0.5, zorder=0)\n",
    "\n",
    "ax = plt.subplot2grid([2, 2], [0, 1], rowspan=2, colspan=1, projection=\"3d\")\n",
    "ax.scatter(input_var[:, 0], input_var[:, 1], output_var)\n",
    "ax.set_xlabel(r\"input 1\"); ax.set_ylabel(r\"input2\"); ax.set_zlabel(r\"output\"); ax.grid(lw=0.5, zorder=0)\n",
    "ax.view_init(15, -45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> Before you really go into stuff you should think about whether something might work or not. So with the graph above, can you say why the procedure below might actually work that well? Could you have anticipated this with information further above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do the regression in raw `numpy` and `scipy`, but we are just going to leverage the `sklearn.linear_model` package, which generically gives you a linear regression for arbitrary number of inputs (so it includes the straight forward linear regression as a special case). \n",
    "\n",
    "The syntax is below, where I use both the bill attributes as inputs and the body mass as output. Once I've fit the model, I get the parameters I need, and I could throw the input values back in to get a **predicted value** to see how well I do compared to the actual data value, which are also given below.\n",
    "\n",
    "> NOTE: The below syntax is good for doing the predictions one at a time (controlled through the `ind` variable), and you can play around with this and see what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "lin_regr = linear_model.LinearRegression() # initialise class\n",
    "\n",
    "input_var = adelie_data[['bill_length_mm', 'bill_depth_mm']].values\n",
    "output_var = adelie_data['body_mass_g'].values\n",
    "\n",
    "lin_regr.fit(input_var, output_var)        # do multilinear fitting\n",
    "\n",
    "# the lin_regr is now your model of y = c + \\Sum a_i * X_i, pull out the coefficients a_i\n",
    "s, c = lin_regr.coef_, lin_regr.intercept_  # (s, c) interpretation as before, see lec03 notebook\n",
    "                                            # the s ar the (linear) trends\n",
    "print(f\"suggested model is y = {c:.4f} + {s[0]:.4f} * x_1 + {s[1]:.4f} * x_2\")\n",
    "print(f\"   r^2 value is {lin_regr.score(input_var, output_var):.4f}\")  # suggestion of fit\n",
    "\n",
    "ind = 0\n",
    "pred_out = lin_regr.predict([[input_var[ind, 0], input_var[ind, 1]]])\n",
    "print(f\"input1 = {input_var[ind, 0]}, input2 = {input_var[ind, 1]}, actual value = {output_var[ind]}, pred value = {pred_out[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> What does the $r$-squared value mean? And given the $r$-squared value, how well do you expect the regression model to perform generally?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is syntax to make the prediction over the range of entries we have, and dumping it back into a Pandas array for easier comparison in a table, as well as plotting the predictions out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adelie_data_mod = adelie_data.copy(deep=True)\n",
    "\n",
    "# make a prediction\n",
    "N = input_var.shape[0]\n",
    "pred_out = np.zeros(N)\n",
    "for ind in range(N):\n",
    "    pred_out[ind] = lin_regr.predict([[input_var[ind, 0], input_var[ind, 1]]])[0]\n",
    "\n",
    "# now have an array of predicted values, put this into pandas to read\n",
    "# syntax: df[\"whatever you want to call the column\"] = data_array\n",
    "adelie_data_mod[\"pred.body_mass_g\"] = pred_out\n",
    "adelie_data_mod[\"signed mismatches\"] = pred_out - adelie_data_mod[\"body_mass_g\"]\n",
    "adelie_data_mod[\"squared mismatch\"] = (pred_out - adelie_data_mod[\"body_mass_g\"]) ** 2\n",
    "adelie_data_mod.sample(10).style.set_properties(**{'background-color': 'pink'}, \n",
    "                                     subset=['pred.body_mass_g', 'signed mismatches', 'squared mismatch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax.scatter(input_var[:, 0], output_var, label=\"raw value\")\n",
    "ax.scatter(input_var[:, 0], adelie_data_mod[\"pred.body_mass_g\"], label=\"pred value\")\n",
    "ax.set_xlabel(r\"input 1\"); ax.set_ylabel(r\"output\"); ax.grid(lw=0.5, zorder=0); ax.legend();\n",
    "\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "ax.scatter(input_var[:, 1], output_var, label=\"raw value\")\n",
    "ax.scatter(input_var[:, 1], adelie_data_mod[\"pred.body_mass_g\"], label=\"pred value\")\n",
    "ax.set_xlabel(r\"input 2\"); ax.set_ylabel(r\"output\"); ax.grid(lw=0.5, zorder=0); ax.legend();\n",
    "\n",
    "ax = plt.subplot2grid([2, 2], [0, 1], rowspan=2, colspan=1, projection=\"3d\")\n",
    "ax.scatter(input_var[:, 0], input_var[:, 1], output_var, label=\"raw value\")\n",
    "ax.scatter(input_var[:, 0], input_var[:, 1], adelie_data_mod[\"pred.body_mass_g\"], label=\"pred value\")\n",
    "ax.set_xlabel(r\"input 1\"); ax.set_ylabel(r\"input2\"); ax.set_zlabel(r\"output\"); ax.grid(lw=0.5, zorder=0); ax.legend();\n",
    "ax.view_init(15, -45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> Are the prediction skill (or lack or) consistent with the information so far?\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Try this with different combinations of attributes, or use data from different species, as form of practice and to see if certain attributes work better. You could also try including more inputs (but see below exercise).\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (technical) Work out how you can get rid of the `for` loop above for the predicton step and just pass in an array so that the model can give the predictions with one single call.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (technical) Python usually makes `soft` copies, and I overrode that with the command `DATA_FRAME.copy(deep=True)`. Look up the difference between soft and hard copying in Python and convince yourself why in this case you might want a deep copy instead. Provide a simple demonstration on the effects of soft copying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "\n",
    "# b) A/BIC: mismatch vs. complexity\n",
    "\n",
    "So lets suppose we repeat the above multi-linear regression for adelie data for simplicity, and we are still interested in predicting the petal length. We could construct a variety of ensembles and evalute its *skill* through their ability to reduce mismatches.\n",
    "\n",
    "The code below \"trains\" up four models with multi-linear regression, and computes the $\\ell_2$ mismatches.\n",
    "\n",
    "> NOTE: I am using all the data, and sometimes when regressing / training a model you don't want to do that, because you want some data left over to evaluate model performance. Here I am delibrately not doing this to make a point and to keep everything deterministic (separating out training and testing data is normally random / stochastic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_var = adelie_data['body_mass_g'].values\n",
    "\n",
    "N = len(output_var)\n",
    "\n",
    "# train up a variety of models\n",
    "\n",
    "input_var = adelie_data[['flipper_length_mm']].values\n",
    "model1 = linear_model.LinearRegression()\n",
    "model1.fit(input_var, output_var)\n",
    "num_coeff1 = len(model1.coef_)+1  # Q. why +1?\n",
    "pred1 = model1.predict(input_var)\n",
    "var1 = np.sum((pred1 - output_var) ** 2) / N\n",
    "\n",
    "input_var = adelie_data[['flipper_length_mm', 'bill_length_mm']].values\n",
    "model2 = linear_model.LinearRegression()\n",
    "model2.fit(input_var, output_var)\n",
    "num_coeff2 = len(model2.coef_)+1\n",
    "pred2 = model2.predict(input_var)\n",
    "var2 = np.sum((pred2 - output_var) ** 2) / N\n",
    "\n",
    "input_var = adelie_data[['flipper_length_mm', 'bill_length_mm', 'bill_depth_mm']].values\n",
    "model3 = linear_model.LinearRegression()\n",
    "model3.fit(input_var, output_var)\n",
    "num_coeff3 = len(model3.coef_)+1\n",
    "pred3 = model3.predict(input_var)\n",
    "var3 = np.sum((pred3 - output_var) ** 2) / N\n",
    "\n",
    "input_var = adelie_data[['bill_depth_mm']].values\n",
    "model4 = linear_model.LinearRegression()\n",
    "model4.fit(input_var, output_var)\n",
    "num_coeff4 = len(model4.coef_)+1\n",
    "pred4 = model4.predict(input_var)\n",
    "var4 = np.sum((pred4 - output_var) ** 2) / N\n",
    "\n",
    "print(f\"error variance (mean squared error) for model 1 is {var1:.6f}, number of coeffs = {num_coeff1}\")\n",
    "print(f\"error variance (mean squared error) for model 2 is {var2:.6f}, number of coeffs = {num_coeff2}\")\n",
    "print(f\"error variance (mean squared error) for model 3 is {var3:.6f}, number of coeffs = {num_coeff3}\")\n",
    "print(f\"error variance (mean squared error) for model 4 is {var4:.6f}, number of coeffs = {num_coeff4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> Go through the code and describe how are the models different.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Suppose you evaluate model performance instead using the sum of the absolute errors instead, is the result the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just from evaluating model skill you would think in this case the third model is the best since that has the lowest $\\ell_2$ mismatch. One could however argue this is cheating somewhat, because model 3 has the most complexity with more input variables than the rest, and thus susceptible to over-fitting effects. A goal here might be to find the simplest model that is the most skillful in some sense, which would indicate the input you chose are particularly **strong predictor** for the variable in question.\n",
    "\n",
    "We could evaluate the skill of a model by its ability to reduce mismatches but penalising the complexity. Two common ways for doing this with statistical models is the **Aikake Information Criterion (AIC)** and the **Bayesian Information Criterion (BIC)** (or sometimes the **Schwarz Information Criterion**), that are based on information and probability theory. For the number of parameter $k$, the sample size $N$ and the (unadjusted) error variance $\\sigma^2 = \\ell_2 / N$, we have respectively (taking $\\log$ to mean $\\log_n = \\ln$, log with the natural base $\\mathrm{e}$)\n",
    "\n",
    "\\begin{equation*}\n",
    "    {\\rm AIC} = 2k + N\\log\\sigma^2, \\qquad {\\rm BIC} = k\\log N + N\\log\\sigma^2.\n",
    "\\end{equation*}\n",
    "\n",
    "The higher the mismatch, the larger the A/BIC (because both scale with $\\log\\sigma^2$ and the $\\log$ function is monotonically increasing), while the lower the complexity as measured by $k$, the smaller the A/BIC. The lower the AIC or BIC, the \"better\" the model is, so provides a means for model selection.\n",
    "\n",
    "> NOTE: A/BIC only gives you the choice to compare between different models trained with the **same** data really. For example, a model of penguins with a lower AIC score compare to the model of cursedness of animal say of the above does not mean anything by itself, because it is not really comparing like for like.\n",
    "\n",
    "> NOTE: If you look up the standard definition of AIC and BIC you will see it is $-2\\log \\hat{L}$ instead, which is the **log-likelihood**. Here I have assumed Gaussian statistics (see *05_statistical_tests*), so I know the likelihood function $\\hat{L}$, and can manipulate the expression to something I already have data for.\n",
    "\n",
    "> NOTE: I am also probably off by 1 in how I am going to count $k$ (because I should probably count the stochasticity as an extra parameter), and there are some multiplicative factors missing from above note, but at least I am consistently wrong here (so it doesn't matter since A/BIC are really relative measures)...\n",
    "\n",
    "The below code provides subroutines for A/BIC and applies it to the models above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_aic(k, variance, N):\n",
    "    aic = 2 * k         + N * np.log(variance)\n",
    "    return aic\n",
    "\n",
    "def calc_bic(k, variance, N):\n",
    "    bic = k * np.log(N) + N * np.log(variance)\n",
    "    return bic\n",
    "\n",
    "print(\"===============================\")\n",
    "print(\"AIC of models\")\n",
    "print(\"===============================\")\n",
    "print(f\"model 1: AIC = {calc_aic(N, var1, num_coeff1):.6f}\")\n",
    "print(f\"model 2: AIC = {calc_aic(N, var2, num_coeff2):.6f}\")\n",
    "print(f\"model 3: AIC = {calc_aic(N, var3, num_coeff3):.6f}\")\n",
    "print(f\"model 4: AIC = {calc_aic(N, var4, num_coeff4):.6f}\")\n",
    "print()\n",
    "print(\"===============================\")\n",
    "print(\"BIC of models\")\n",
    "print(\"===============================\")\n",
    "print(f\"model 1: BIC = {calc_bic(N, var1, num_coeff1):.6f}\")\n",
    "print(f\"model 2: BIC = {calc_bic(N, var2, num_coeff2):.6f}\")\n",
    "print(f\"model 3: BIC = {calc_bic(N, var3, num_coeff3):.6f}\")\n",
    "print(f\"model 4: BIC = {calc_bic(N, var4, num_coeff4):.6f}\")\n",
    "\n",
    "# so from these measures, model 4 is actually the best\n",
    "\n",
    "# Q. try these things for different combinations of variables and/or species of flowers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, both AIC and BIC suggests model 4 is actually the best out of the four. This quantities are only indicative, so some judgement has to be made to which models to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> When in doubt which one to use, compute both to give yourself a second opinion (AIC is normally used I think).\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> I haven't been exhaustive in the choice of variables input/output, try a few of these yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "# c) Principal Component Analysis (PCA)\n",
    "\n",
    "Last one we are going to deal with, and it is related to A/BIC in the sense that you want to try and pick out the *most important* feautres in a dataset, whatever \"most important\" actually means. \n",
    "\n",
    "A common way to do is to carry out a **principal compoent analysis (PCA)**, where you are essentially trying to select most important features (the **principal components (PC)**) ranked by the **variance explained**. This is useful for preliminary data exploration, for reducing the dimension of data, to see basic trends and/or clustering of data, and can also be used for scrambling the dataset for confidentiality reasons etc. \n",
    "\n",
    "Picking out the most important features might be of particular interest when modelling, such as statistical modelling and/or machine learning, where you don't want to have a model that is overly complex (and more susceptible to over-fitting), but still want to capture the essence of the data. The example below shows how PCAs could be applied onto pictures of cats and dogs (right, first 4 PCs of data; from Fig 10 of [Brunton, Brunton, Proctor & Kutz (2013)](https://arxiv.org/pdf/1310.4217.pdf)), picking out features that are common to both (resulting in some cursed catdogs?), which has applications in image recognition for example.\n",
    "\n",
    "<img src=\"https://i.imgur.com/D5TJanm.png\" width=\"800\" alt='brunton_et_al_13_fig10'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first go through what PCA actually does for a trivial example in two dimensions (so with two variables), demonstrating what the PCA is actually doing for a trivial case of projection onto one dimension, before applying it to the full Penguins data (which is four dimensional data, in the sense that there are four attributes with numerical values that are documented).\n",
    "\n",
    "> NOTE: We are going to revisit the point about decomposing and projecting data again in *07_time_series* when dealing with the Fourier basis, and in *10_fun_with_maps* when we are dealing with **Empricial Orthogonal Functions (EOFs)** for spatial-temporal data (which is closely related to PCs). Look ahead a bit if you like, or Wikipedia can help you.\n",
    "\n",
    "PCA is when you find the a combination of variables such that the variance explained is maximised. For the penguins data, while the data collected is ***bill length*** and ***body mass*** data, it might be that some weird combination of (say) ***bi-dy mass*** and ***bo-ll length*** is in fact better at capturing the essence of data. The former are the original variables, while the latter would be the **principal components**. \n",
    "\n",
    "If you are willing to believe in mathemagics, the code below explains visually what is happening somewhat: you are essentially trying to find an ellipsoid of sorts that \"minimallly\" encompasses the data. You can then take the ellipsoid axis as the new axis (spanned by the principal components essentially) and transform the data into something that is circular in the new space. You could do this in raw `numpy` (see exercise below), but I am just going to use the one in `sklearn` to do it.\n",
    "\n",
    "> NOTE: Those with maths background, you are defining a new orthogonal basis spanned by the eigenvectors of the covariance matrix (which is necessarily symmetric). The transformed covariance matrix in the new basis is diagonal and the eigenvalues are the variance explained, ranked accordingly by magnitude; ranking is guaranteed, because the matrix is real and symmetric (so is Hermitian), and thus have real eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from 2d to 2d\n",
    "\n",
    "The code below does a PCA for two variables, with 2 PCs, which will essentially going to be a rotation and stretch of the data to make it circular in the PC co-oridinates. While you get a more covenient representation of data that is as circular as possible in a sense, you lose the interpretability because the PCs don't necessarily correspond to things that are that obvious (since it is some combination of the original variables, in this case `bill_length_mm` and `bill_depth_mm`.\n",
    "\n",
    "I didn't normalise the data (and you should; see next code cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var = adelie_data[['bill_length_mm', 'bill_depth_mm']].values\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)   # define a PCA object with two PCAs (can choose more for larger data)\n",
    "pca.fit(input_var)\n",
    "\n",
    "print(f\"pca components    = {pca.components_}\")\n",
    "print(f\"pca var explained = {pca.explained_variance_}\")\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.scatter(input_var[:, 0], input_var[:, 1], c='k')\n",
    "ax.scatter(pca.mean_[0], pca.mean_[1], c='r', marker='x')\n",
    "ax.annotate(\"\", pca.mean_ + pca.components_[0] * 2 * np.sqrt(pca.explained_variance_[0]), pca.mean_,\n",
    "            arrowprops=dict(color=\"green\", arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0))\n",
    "ax.annotate(\"\", pca.mean_ + pca.components_[1] * 2 * np.sqrt(pca.explained_variance_[1]), pca.mean_,\n",
    "            arrowprops=dict(color=\"blue\", arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0))\n",
    "ax.set_xlabel(r\"input 1\"); ax.set_ylabel(r\"input 2\"); ax.grid(lw=0.5, zorder=0)\n",
    "\n",
    "input_pca = pca.transform(input_var)\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.scatter(input_pca[:, 0], input_pca[:, 1], c=\"k\")\n",
    "ax.annotate(\"\", [2 * np.sqrt(pca.explained_variance_[0]), 0], [0, 0],\n",
    "            arrowprops=dict(color=\"green\", arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0))\n",
    "ax.annotate(\"\", [0, 2 * np.sqrt(pca.explained_variance_[1])], [0, 0],\n",
    "            arrowprops=dict(color=\"blue\", arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0))\n",
    "ax.set_xlabel(r\"PC 1\"); ax.set_ylabel(r\"PC 2\"); ax.grid(lw=0.5, zorder=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA components above tell you the combination of input 1 and 2 you should take to define the PCs. The variance explained is the *raw* value of the variance; in this case PC1 explains most of the variance, while PC2 not so much (which may not be surprising when you look at the green arrow on the left plot and the shape of the data scatter).\n",
    "\n",
    "The above is not right because PCA requires scaling the data by demeaning and scaling it down to the unit interval (cf. Z-scores; see *05_statistical_tests*). One extra here does the standard scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "input_scaled = scaler.fit_transform(input_var)\n",
    "\n",
    "pca = PCA(n_components=2)   # define a PCA object with two PCAs (can choose more for larger data)\n",
    "pca.fit(input_scaled)\n",
    "\n",
    "print(f\"pca components    = {pca.components_}\")\n",
    "print(f\"pca var explained = {pca.explained_variance_}\")\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.scatter(input_scaled[:, 0], input_scaled[:, 1], c='k')\n",
    "ax.scatter(pca.mean_[0], pca.mean_[1], c='r', marker='x')\n",
    "ax.annotate(\"\", pca.mean_ + pca.components_[0] * 2 * np.sqrt(pca.explained_variance_[0]), pca.mean_,\n",
    "            arrowprops=dict(color=\"green\", arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0))\n",
    "ax.annotate(\"\", pca.mean_ + pca.components_[1] * 2 * np.sqrt(pca.explained_variance_[1]), pca.mean_,\n",
    "            arrowprops=dict(color=\"blue\", arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0))\n",
    "ax.set_xlabel(r\"input 1\"); ax.set_ylabel(r\"input 2\"); ax.grid(lw=0.5, zorder=0); ax.axis(\"equal\");\n",
    "\n",
    "input_pca = pca.transform(input_scaled)\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.scatter(input_pca[:, 0], input_pca[:, 1], c=\"k\")\n",
    "ax.annotate(\"\", [2, 0], [0, 0],\n",
    "            arrowprops=dict(color=\"green\", arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0))\n",
    "ax.annotate(\"\", [0, 2], [0, 0],\n",
    "            arrowprops=dict(color=\"blue\", arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0))\n",
    "ax.set_xlabel(r\"PC 1\"); ax.set_ylabel(r\"PC 2\"); ax.grid(lw=0.5, zorder=0); ax.axis(\"equal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we scaled the data we might want to re-scale the data. The code below undoes the scaling, for both the original case (so this is just the original data), and for the PCs (which is the PC in \"real\" units, not that the PC itself is necessarily overly information from a physical point of view).\n",
    "\n",
    "> NOTE: We need the `pca.mean_.reshape(1, -1)` for syntax reasons, as the inverse transform is expecting two arguments but as a \"column\" rather stacked as a \"row\" (I am not saying I agree with it, I am just doing what the routine wants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_mean = scaler.inverse_transform(pca.mean_.reshape(1, -1))\n",
    "unscaled_pca_input = scaler.inverse_transform(input_pca)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.scatter(input_var[:, 0], input_var[:, 1], c='k')\n",
    "ax.scatter(unscaled_mean[0, 0], unscaled_mean[0, 1], c='r', marker='x')\n",
    "ax.set_xlabel(r\"input 1\"); ax.set_ylabel(r\"input 2\"); ax.grid(lw=0.5, zorder=0); ax.axis(\"equal\");\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.scatter(unscaled_pca_input[:, 0], unscaled_pca_input[:, 1], c=\"k\")\n",
    "ax.set_xlabel(r\"PC 1\"); ax.set_ylabel(r\"PC 2\"); ax.grid(lw=0.5, zorder=0); ax.axis(\"equal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from 2d to 1d\n",
    "\n",
    "In the above case this is essentially a co-ordinate transformation because I have two input data and I asked for 2 PCs. To sort of see what it is doing in a rather trivial (but illuminating, at least for me) way, we are going to instead ask for 1 PC. The below code does the scaling, the PCA, and re-scales as well as reverts the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_var = adelie_data[['bill_length_mm', 'bill_depth_mm']].values\n",
    "scaler = StandardScaler()\n",
    "input_scaled = scaler.fit_transform(input_var)\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(input_scaled)\n",
    "\n",
    "print(f\"pca components    = {pca.components_}\")\n",
    "print(f\"pca var explained = {pca.explained_variance_}\")\n",
    "\n",
    "input_pca = pca.transform(input_scaled)\n",
    "proj_scaled = pca.inverse_transform(input_pca)\n",
    "proj = scaler.inverse_transform(proj_scaled)\n",
    "unscaled_mean = scaler.inverse_transform(pca.mean_.reshape(1, -1))\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = plt.axes()\n",
    "ax.scatter(input_var[:, 0], input_var[:, 1], c='k')\n",
    "ax.scatter(proj[:, 0], proj[:, 1], c='b')\n",
    "ax.scatter(unscaled_mean[0, 0], unscaled_mean[0, 1], c='r', marker='x')\n",
    "ax.annotate(\"\", scaler.inverse_transform(pca.mean_ + pca.components_ * 2 * np.sqrt(pca.explained_variance_))[0], \n",
    "            scaler.inverse_transform(pca.mean_.reshape(1, -1))[0],\n",
    "            arrowprops=dict(color=\"red\", arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0))\n",
    "ax.set_xlabel(r\"input 1\"); ax.set_ylabel(r\"input 2\"); ax.grid(lw=0.5, zorder=0); ax.axis(\"equal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is happening is that you take the data, perform the PCA with 1 PC, so this is just the green arrow previously. Then what you are doing is taking the data and **projecting** it onto the PC, i.e. onto the red line here, so the result is that you only get data on a line. While your data in the original variables is still two dimensional in the sense there are two outputs, the data could really be regarded as lower dimensional (in this case 1d), because it basically follows a line (which can be represented by one number, the length away from whatever point on the line you decide.\n",
    "\n",
    "> NOTE: In maths speak, the line can be parameterised by one variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from 4d to 2d\n",
    "\n",
    "Well you might ask what is the point of that? So in the penguin data you have data in 4d (`bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`), so how do you visualise all the data at the same time, or highlight what are the important features? We don't have enough dimensions to represent all variables at the same time (you could do 3d plots but then you only include 3 variables at a time), but in some sense you could by doing a PCA.\n",
    "\n",
    "The codes below does this for the whole penguin data. We first do the zeroth rule of data analysis and just do a sample plot to see what the data looks like. I don't know about you, but I can't see anything here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and redfine data for safety\n",
    "\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"penguins.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES3301_data_analysis/refs/heads/main/penguins.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "adelie_data = df.loc[df[\"species\"] == \"Adelie\"]\n",
    "chinst_data = df.loc[df[\"species\"] == \"Chinstrap\"]\n",
    "gentoo_data = df.loc[df[\"species\"] == \"Gentoo\"]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "key1, key2, key3 = \"bill_length_mm\", \"bill_depth_mm\", \"body_mass_g\"\n",
    "\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.scatter(adelie_data[key1], adelie_data[key2], adelie_data[key3], label=\"adelie\")\n",
    "ax.scatter(chinst_data[key1], chinst_data[key2], chinst_data[key3], label=\"chinstrap\")\n",
    "ax.scatter(gentoo_data[key1], gentoo_data[key2], gentoo_data[key3], label=\"gentoo\")\n",
    "ax.set_xlabel(f\"{key1}\")\n",
    "ax.set_ylabel(f\"{key2}\")\n",
    "ax.set_zlabel(f\"{key3}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend()\n",
    "ax.view_init(25, -45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do a PCA keeping 2 PCs, which results in a formally 2d data, given in a table and a 2d scatter plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "input_vars = df.loc[:, features].values\n",
    "\n",
    "input_var_scaled = StandardScaler().fit_transform(input_vars)\n",
    "\n",
    "pca = PCA(n_components=2)   # define a PCA object with two PCAs (can choose more for larger data)\n",
    "pca_comp = pca.fit_transform(input_var_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(data = pca_comp, columns = ['PC1', 'PC2'])\n",
    "pca_df[\"species\"] = df.loc[:,['species']].values  # add the variety column back on\n",
    "pca_df.sample(10)                                 # this is now 2d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 5))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "for name in [\"Adelie\", \"Chinstrap\", \"Gentoo\"]:\n",
    "    ax.scatter(pca_df.loc[df[\"species\"] == name][\"PC1\"], \n",
    "               pca_df.loc[df[\"species\"] == name][\"PC2\"], label = name)\n",
    "ax.set_xlabel(f\"PC1 (var explained = {pca.explained_variance_ratio_[0] * 100 : .2f}%)\")\n",
    "ax.set_ylabel(f\"PC2 (var explained = {pca.explained_variance_ratio_[1] * 100 : .2f}%)\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "pca.fit_transform(input_var_scaled)\n",
    "pca.components_\n",
    "# pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCs are ranked by the variance explained, and in this case it is saying the two PCs together explain over 95% of the data variance, so that's good. We also see here that the three species of penguins do form clusters, with the gentoo data noticeably different from the other two, and the other two have some overlap. \n",
    "\n",
    "The way you might want to intrepret this is that the clustering suggests that species is a good predictor for the attributes, i.e. if you know the species, you can probably construct a model to predict the attributes, but inverting the 2 PCs in this case to your 4 original variables (because you have the mapping to go between the two representations). \n",
    "\n",
    "You can go the other way as well, so you could regress in PC-space, invert the model, and that gives you some representation of your regressed model in the original variables; technically in this case the complexity you would count the number of free variables, which really is 2 (in PC-space). Not going to do this here unless you want a challenge (see below for suggested exercise).\n",
    "\n",
    "Below code outputs the raw variance and variance ratio explained by the PCs. The 3rd and 4th PC explains very little (consistent with the large variance already explained by the first two PCs), so it's probably fine dropping those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_explained = np.zeros((4, 4))\n",
    "var_explained_ratio = np.zeros((4, 4))\n",
    "for i in range(1, 5):\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit_transform(input_var_scaled)\n",
    "    for j in range(i):\n",
    "        var_explained[i-1, j] = pca.explained_variance_[j]\n",
    "        var_explained_ratio[i-1, j] = pca.explained_variance_ratio_[j]\n",
    "\n",
    "print(\"===============================\")\n",
    "print(\"raw value of variance explained\")\n",
    "print(\"===============================\")\n",
    "print(var_explained)\n",
    "print()\n",
    "print(\"===============================\")\n",
    "print(\"percentage of variance explained\")\n",
    "print(\"===============================\")\n",
    "print(var_explained_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can invert the mapping by convert the two PCs back to the original four real variables, and see the consequences on the data. The code below gives a demonstration of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "input_vars = df.loc[:, features].values\n",
    "\n",
    "# scale the data\n",
    "input_var_scaled = scaler.fit_transform(input_vars)\n",
    "\n",
    "pca = PCA(n_components=2)   # define a PCA object with two PCAs (can choose more for larger data)\n",
    "pca_comp = pca.fit_transform(input_var_scaled)\n",
    "\n",
    "proj = scaler.inverse_transform(pca.inverse_transform(pca_comp))\n",
    "\n",
    "df[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]] = proj\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "# More involved exercises with this notebook\n",
    "\n",
    "## 1) Iris data\n",
    "\n",
    "The [iris data](https://en.wikipedia.org/wiki/Iris_flower_data_set) was originally compiled by Ronald Fisher and is generally regarded as the *hello_world* of data analysis (and machine learning as well). Do a similar analysis as the penguin data but for this dataset. You should read open the source file and go to that website to have a look and see what variables are there etc. This is available here as `iris.csv` (the link is here: https://raw.githubusercontent.com/julianmak/OCES3301_data_analysis/refs/heads/main/iris.csv).\n",
    "\n",
    "> NOTE: Since this is a well-known dataset, a lot of the answers you might want are actually readily found on Google.\n",
    "\n",
    "> Random unrelated fact: cats are quite allergic to iris flowers (and quite a lot of flowers actually), so keep them away\n",
    "\n",
    "I touched up the iris data from the [UCL repository](https://archive.ics.uci.edu/ml/datasets/iris) and dumped it out into a text file called `iris.csv`, which should be able to be read into Pandas directly. Try and explore the Iris data a bit and do some statistics/plots on it (I am not going to describe the data, you can look this up on the Wikipedia above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) A/BIC practice\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Consider applying A/BIC to the example in *03_regression* when you go back to doing linear regression (with some fix choice of data; you could use my linear/quadratic example or cook something up) instead varying the degree of the polynomial (for simplicity you could take the number of coefficients to be the measure of complexity). Graph out the A/BIC as well as contributions to the A/BIC as well to see how these change with increasing complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) PCA et al.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (hard-ish) Write your own PCA code in numpy (you will need `np.cov` and the linear algebra package; the internet is your friend here). You have the right answer to check again in the form of the code above, and some data you already analysed.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Read up what a **linear discriminant analysis (LDA)** is, how it is used, and apply it to the penguin and/or the iris data.\n",
    ">\n",
    "> May be useful for the next excercise should you wish to attempt it.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (hard-ish and tedious) This is probably related **clustering algorithms** but much more primitive, and is sort of like a machine learning by hand exercise. Given the PC-mapping, consider coding up something that, when given the four attributes of the penguin or iris data, returns whether the predicted flower variety. So it's also like a multi-linear model but it gives you discrete instead of continuous outcomes.\n",
    ">\n",
    "> If I were to do this, I would probably chop out a proportion of data for training and testing purposes, given there are 50 entries per species of flower, probably 40 for traning and 10 for testing. I would throw in the 40 pieces of data for doing a PCA and, looking at the reuslts, cook up by hand some criteria (in PC-space) that returns some outcomes to me, such as\n",
    ">\n",
    ">```\n",
    ">    if (data_PC1 > blah) & (data_PC1 < bleh) & (data_PC2 > blih) & (data_PC2 < bluh):\n",
    ">      print(f\"probably setosa\")\n",
    ">    elif (...)\n",
    ">      print(f\"probably virginica\")\n",
    ">    elif (...\n",
    ">      print(f\"probably versicolor\")\n",
    ">    else:\n",
    ">      print(f\"out of bounds, who knows?\")\n",
    ">```\n",
    "> and pass the remaining data in and predict the flower variety, and see how well you do.\n",
    ">\n",
    "> For extra work, try and look up and see if `sklearn` already has something that can do this kind of thing (there probably is, and this exercise is really to get you to think algorithmically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Generalised Linear/Additive Models\n",
    "\n",
    "(This part might make more sense after having gone through `05/06_statistical_model`)\n",
    "\n",
    "[***Generalised Linear Models***](https://en.wikipedia.org/wiki/Generalized_linear_model) could be regarded as a generalisation to linear regression by providing an extra component that enables more flexible fitting of data. This is provided through the [`statsmodel`](https://www.statsmodels.org/dev/gam.html) package in Python, but is probably more thorough in R. Have a look at it and try a few things with it.\n",
    "\n",
    "(This part requires a bit more exposure to concepts in probability, so is not an examined part of this course.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
