{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*updated 18 Jan 2025, Julian Mak (whatever with copyright, do what you want with this)\n",
    "\n",
    "### As part of material for OCES 3301 \"Data Analysis in Ocean Sciences\" delivered at HKUST\n",
    "\n",
    "For the latest version of the material, go to the public facing [GitHub](https://github.com/julianmak/academic-notes/tree/master/OCES3301_data_analysis_ocean) page."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02: Python data reading and some data manipulation\n",
    "\n",
    "While a good portion of this course uses data generated on the fly (e.g. see *assignment 1* and some samples from last session), \"real\" data is usually obtained from observations / numerical experiments / lab work that may be costly or not worthwhile repeating. Data is usually stored somewhere, and we want to be able to read it in order to do something with it.\n",
    "\n",
    "Most data we are going to deal with here are going to be **text** format (e.g. with the *txt* or *csv* extension, though sometimes no extension at all), and towards the end of the course we are going to start reading some **binary** format data (in this case in the [netCDF](https://pro.arcgis.com/en/pro-app/latest/help/data/multidimensional/what-is-netcdf-data.htm) format, which is common in geophysical data). This is of course only representative, and while the reading commands might change, the analysis part should be largely similar.\n",
    "\n",
    "Before we go on and as promised in the previous session, here is a picture of the cursed frog:\n",
    "\n",
    "<img src=\"https://i.imgur.com/5qhT8OK.jpg\" width=\"400\" alt='cursed frog'>\n",
    "\n",
    "> ## Key Objective(s)\n",
    "> 1. Basic data reading and manipulations\n",
    "> 2. Introduce the use of `pandas` to read/manipulate data.\n",
    "> 3. Recall some basic statistical concepts and calculate these using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some default packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "\n",
    "# a) Reading text-based data\n",
    "\n",
    "Python natively will read text files. The file I made is in `rick_roll.txt`, which you could try and read it outside of Python (it's just a text file, so something like notepad will do it). To read it in Python, here are the somewhat old-fashioned (but very robust) commands you can do.\n",
    "\n",
    "> NOTE: By default in this course I will be loading data \"remotely\", and this requires an internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    f = open(\"rick_roll.txt\", \"r\") # r/w/a = read/write/append, so here is \"read only\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES3301_data_analysis/refs/heads/main/rick_roll.txt\"\n",
    "    f = urllib.request.urlopen(path)\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "print(f)\n",
    "\n",
    "f.close()  # remember to close the file otherwise weird things might happen..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So notice here none of the contents of the file actually got print to screen, because what is going on here is that a **pointer** (in this case the variable `txt_file`) is made to the file, providing an access point of sorts to do things to the file, until it is closed (through `txt_file.close()`). So no reading was actually done because no reading commands were actually issued, which in this case is done with `txt_file.readlines()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    f = open(\"rick_roll.txt\", \"r\") # r/w/a = read/write/append, so here is \"read only\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES3301_data_analysis/refs/heads/main/rick_roll.txt\"\n",
    "    f = urllib.request.urlopen(path)\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "txt_src = f.readlines()\n",
    "\n",
    "f.close()  # remember to close the file otherwise weird things might happen...\n",
    "\n",
    "for k in range(len(txt_src)):\n",
    "    if type(txt_src[k]) == str:\n",
    "        txt_src[k] = txt_src[k].strip(\"\\n\")\n",
    "    elif type(txt_src[k]) == bytes:\n",
    "        txt_src[k] = txt_src[k].decode(\"utf-8\").strip(\"\\n\")\n",
    "\n",
    "print(txt_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: `readlines()` is a routine associated to the `txt_file` object, and `txt_file.readlines()` here is saying use the readlines command on itself. The reason for this kind of arrangement (called **object oriented programming**) is that in principle you pack the data together with the routines, so for example you can avoid calling routines to act on stuff that the routine was never designed for (e.g. the computer doesn't necessarily understand what calculating the variance means for a string?) Python is an **object-oriented** language.\n",
    "\n",
    "Note that we closed the pointer to the file so references to `txt_file` should no longer work, but the data has been dumped out to `txt_src`, which is now the list of strings we can play with.\n",
    "\n",
    "> NOTE: I stripped out the `\\n` already (it stands for \"new line\"), with the `.strip` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(txt_src)):\n",
    "    print(txt_src[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could replace things in a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_src[-1].replace(\"Never\", \"Always\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> what is the `-1`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing to a file is similar: we open a pointer (this time with `w` for read), use the pointers `.write()` command to write a line, and when we are done, close it.\n",
    "\n",
    "> NOTE: you need the `\\n` to tell Python to write a new line. Here I did this by adding `\\n` to every line being written.\n",
    "\n",
    "Writing would generally be useful for example if there is some data you analysed, and re-running the code will take a while, or you want to share the processed results with someone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = open(\"rick_roll_troll.txt\", \"w\")\n",
    "for i in range(len(txt_src)):\n",
    "    txt_file.write(txt_src[i].replace(\"Never\", \"Always\") + \"\\n\") \n",
    "txt_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> what is the above code doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below provides an alternative way to opening a file that closes it immediate after reading, through the `with` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rick_roll_troll.txt\", \"r\") as f:\n",
    "    mod_txt = f.readlines()\n",
    "for line in range(len(mod_txt)):\n",
    "    print(mod_txt[line].replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "# b) Reading numeric data but in text format\n",
    "\n",
    "Lets try a slightly less stupid example but one that highlights some generic things to watch out for. The (text) file `elnino34_sst.data` contains sea surface temperature (SST) data over the El-Nino 3.4 region.\n",
    "\n",
    "> NOTE: if you don't know what El-Nino is, go to Wikipedia, OCES 2003 notes, or ENVS 3004 / OCES 4001 notes\n",
    "\n",
    "Lets just open it and see what it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    f = open(\"elnino34_sst.data\", \"r\")\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES3301_data_analysis/refs/heads/main/elnino34_sst.data\"\n",
    "    f = urllib.request.urlopen(path)\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "elnino34_txt = f.readlines()\n",
    "for k in range(len(elnino34_txt)):\n",
    "    if type(elnino34_txt[k]) == str:\n",
    "        elnino34_txt[k] = elnino34_txt[k].strip(\"\\n\")\n",
    "    elif type(elnino34_txt[k]) == bytes:\n",
    "        elnino34_txt[k] = elnino34_txt[k].decode(\"utf-8\").strip(\"\\n\")\n",
    "elnino34_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So while I read the file as a list, the entries we are primarily interested in are numbers, so we want a way to pull the numbers out. Also, note that:\n",
    "\n",
    "1) There are a few unnecessary line at the beginning (the **headers**) and at the end (the **footers**), that we should get rid of probably for the data analysis purposes.\n",
    "\n",
    "2) The year is on the left column, but we probably don't need that.\n",
    "\n",
    "3) It looks like there are 12 columns, which in this case really does correspond to entries to each of the months.\n",
    "\n",
    "> NOTE: I think (but I can't remember) this is actually raw monthly averaged SST values, rather than averaged over a window to give *low-passed* data; see *07_time_series* for what this means if you want.\n",
    "\n",
    "4) there are entries of `-99.99`, which in this case are **missing values** (no data), which you probably want to do something about\n",
    "\n",
    "> NOTE: Sometimes the missing values show up as something else, or marked on differently, so you do need to check for these. How you deal with those is a separate issue (e.g. if the missing value occurs in the middle of some time, do you approximate it as something, fill it with something else, ignore it? The choice can affect the resulting analysis and is really guided by what question you are trying to answer. More on this in `08_time_series`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am going to process this the old-fashioned way to illustrate the points. I am going to\n",
    "\n",
    "1) Strip out by hand the first 3 lines (header and the two lines with the missing values), and the last 4 lines (the footers).\n",
    "\n",
    "2) Read each line, split out the entires somehow, but skip the first one (so not bothering with the year), noting that we are going from 1950 to 2019, and dump out the numbers per read.\n",
    "\n",
    "3) Turn the eventual list into an array.\n",
    "\n",
    "The code below does this, but for demonstration purposes, you should run the individual component separately (in different cells for example) to convince yourself what each part of the code is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    f = open(\"elnino34_sst.data\", \"r\")\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES3301_data_analysis/refs/heads/main/elnino34_sst.data\"\n",
    "    f = urllib.request.urlopen(path)\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "elnino34_txt = f.readlines()\n",
    "elnino34_txt = elnino34_txt[3:-4]\n",
    "for k in range(len(elnino34_txt)):\n",
    "    if type(elnino34_txt[k]) == str:\n",
    "        elnino34_txt[k] = elnino34_txt[k].strip(\"\\n\")\n",
    "    elif type(elnino34_txt[k]) == bytes:\n",
    "        elnino34_txt[k] = elnino34_txt[k].decode(\"utf-8\").strip(\"\\n\")\n",
    "\n",
    "elnino34_txt[0].split() # then we split each line (as a string) up into components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an empty list, cycle through each line, split, and add in the entries\n",
    "elnino34_sst = []\n",
    "for k in range(len(elnino34_txt)):           # this is the new elnino34_txt after stripping out some lines\n",
    "    dummy = elnino34_txt[k].split()          # split out the entries per line\n",
    "    for i in range(1, len(dummy)):           # cycle through the dummy list but skip the first entry\n",
    "        elnino34_sst.append(float(dummy[i])) # turn string into a float, then add to list\n",
    "\n",
    "elnino34_sst = np.asarray(elnino34_sst)      # turn into array (not strictly necessary)\n",
    "print(elnino34_sst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> What is the size of this array (look up how you might check), and is it exactly the same size as you expect it to be (what is the number of entries you should be getting)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a raw plot of this to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(elnino34_sst)\n",
    "ax.set_xlabel(r\"index (not time)\")\n",
    "ax.set_ylabel(r\"SST (${}^\\circ\\ \\mathrm{C}$)\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here I haven't bothered to process the time (so there is no \"time\" to plot against), but it can in principle be generated. I could also have plotted straight from a list, but for data manipulation having an array is easier.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (NEEDS CARE) Create a time vector manually so you can plot the above graph against a proper time showing a year (trying using `np.linspace`; look up syntax via `np.linspace?` or Google, being careful about the END year.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Modify the code above so `elnino34_sst` stays a list, and try taking away a fixed number from it (say do `elnino34_sst - 2`), what happens? Do you get the same results from `elnino34_sst` as an array? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing [pandas](https://pandas.pydata.org/)\n",
    "\n",
    "<img src=\"https://i.imgur.com/rKcpZzr.jpg\" width=\"400\" alt='cursed panda'>\n",
    "\n",
    "The Python Pandas package (not the cursed panda above; longer tutorial [here](https://www.w3schools.com/python/pandas/default.asp)) is a versatile package that is particularly good for packaging data and doing relevant analysis on.\n",
    "\n",
    "Whatever I am going to be doing over for the rest of this course can probably be done through Pandas in principle, but I am mostly not going to be doing things through Pandas, as I think it hides a bit of the detail I think is important to go through somewhat. But an ongoing exercise would be for you do whatever I am doing but in Pandas.\n",
    "\n",
    "Anyway, reading data can be done through the `read_csv` command that comes with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"elnino34_sst.data\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES3301_data_analysis/refs/heads/main/elnino34_sst.data\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "data = pd.read_csv(path)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So generally pandas ***tries*** to read things assuming sensible layout etc., but that can fail if the data is not cleaned up (and uncleaned data is the default rather than the norm). So, it is generally advised to have a look at the raw data file first to see what it consists, anticipate what things you might need to do, and  know back up options (hence somewhat of a reason for the above manipulations in the old-fashioned way).\n",
    "\n",
    "> NOTE: Again, it is probably good to accept that data you will get your hands on is almost never cleaned up, learn what kind of problems can get thrown up and learn how to deal with it, instead of always getting clean data and not knowing what to do when data you get is no longer clean.\n",
    "\n",
    "In this case, optional arguments needs to be provided (e.g. delimiter, separator, etc...). I've done the whole thing below.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Experiment on what happens if you mess around with the options below and what outputs you get (giving you some experience as to what the \"wrong\" things look like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can give it a few more details to make it easier for pandas to help us\n",
    "data = pd.read_csv(path,\n",
    "            sep='\\s+',  # white space as delimiter\n",
    "            names=[\"year\", \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"],\n",
    "            skipfooter=4,  # chop out some lines\n",
    "            skiprows=1,    # chop out some unnecessary lines\n",
    "            false_values=-99.99,\n",
    "            engine=\"python\")\n",
    "data = data.replace(-99.99, np.nan) # replace missing values with NaNs (not a number)\n",
    "data = data.set_index(\"year\")       # sets the index to be the year column\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of the Pandas data frame like a dictionary, so you could for example do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3))\n",
    "\n",
    "# forces pandas to plot on the given ax object, otherwise it generates a new one\n",
    "ax = plt.axes()\n",
    "data[\"Jan\"].plot(ax=ax) \n",
    "\n",
    "# because year is set as index, so data is being plotted against index\n",
    "#  can modify this accordingly\n",
    "ax.set_xlabel(r\"year\")\n",
    "ax.set_ylabel(r\"SST (${}^\\circ\\ \\mathrm{C}$)\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note you do have a time-axis here, because the `year` column was not discarded in this case, but it is of course less obvious how you plot the whole time-series out.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> What is the above actually doing?\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> With a for loop or otherwise, plot the data for all 12 months of data at the same time.\n",
    "\n",
    "The code below is basically the same as above, but here I pull out the data in the Pandas data frame using the `.values` command (so now I am dealing with an array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code is does basically the same as above\n",
    "\n",
    "year = data.index.values\n",
    "sst  = data[\"Jan\"].values\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(year, sst)\n",
    "ax.set_xlabel(r\"year\")\n",
    "ax.set_ylabel(r\"SST (${}^\\circ\\ \\mathrm{C}$)\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> (harder) Try and see if you can bully Pandas into plotting out the whole data array as a complete time-series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "\n",
    "# c) Basic statistics (and some more Pandas)\n",
    "\n",
    "Having a whole load of data is all well and good but we want some way to quantify these, so the rest of the course deals with that. The main kind of data we are going to be deal with are going to be **numbers**, and essentially we are going to be doing calculations for things.\n",
    "\n",
    "For ease of talking, I am going to introduce some terminology and notation that I am not going to be very rigourous in defining. A **random variable** $X$ (capital letters) following some **probability distribution** can be sampled, and I am going to denote the **samples** of the random variable as $x_i$ (small letters, with index $i$ denoting the $i^{\\rm{th}}$ sample).\n",
    "\n",
    "> NOTE: $X$ really is a map from the sample space $\\Omega$ (space of all possible outcomes) to some measurable space $E$, but we don't really need this here...\n",
    "\n",
    "For most intents and purposes, we are going to be dealing with $x_i$, the data, and occasionally the probability distribution function or the random variable, but otherwise you can largely forget about the rest of the stuff I just said."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average\n",
    "\n",
    "So there are really three kinds of *averages*, but usually when we talk about THE average, we really mean the **mean**. We are going to skip that for the moment and talk about the other two first. \n",
    "\n",
    "1) **Mode**, the sample value that occurs the most.\n",
    "\n",
    "2) **Median**, order the samples, and pick the middle one.\n",
    "\n",
    "We will usually not use the mode, but the median will show up when we do the box-and-whisker plots (as the middle line, or the **50th quartile**).\n",
    "\n",
    "`Numpy` has a built in function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = np.asarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "print(f\"median of ages = {np.median(ages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Range and quartiles\n",
    "\n",
    "The **range** of the sample is simply the distance between the smallest and largest value, and is a rather crude measure of spread.\n",
    "\n",
    "The **upper/lower quartiles** or the **75/25 quartile** is also a measure of spread, but for data in the center. The upper quartile is the location above which 25% of the ranked data lies (or location of where 75% of the data lies below), and the lower quarter is analogously defined.\n",
    "\n",
    "These can be respectively done by a simple `max - min`, and for example the `np.percentile` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"range of ages = {np.max(ages)} - {np.min(ages)} = {np.max(ages) - np.min(ages)}\")\n",
    "print(f\"lower quartile = {np.percentile(ages, 25)}\")\n",
    "print(f\"mid   quartile = {np.percentile(ages, 50)}\")\n",
    "print(f\"upper quartile = {np.percentile(ages, 75)}\")\n",
    "print(\"\")\n",
    "print(f\"quartile of ages in one go = {np.percentile(ages, [25, 50, 75])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range and quartiles can be shown in a diagram through a **box-and-whisker diagram** (or a **box plot**). The code below does a box plot, and the features are:\n",
    "\n",
    "* The line in the box, which is the median.\n",
    "* The edges of the box, which are the upper and lower quartiles, and the ranges is called the **inter-quartile range**.\n",
    "* The whiskers, use to denote data outside of the inter-quartile range.\n",
    "* The edges of the whiskers, which denote the range.\n",
    "\n",
    "Sometimes when you use the built in boxplot commands extra circles appear. These usually denote the **outliers**, that are sufficiently far away from the median (usually measured by some factor of the inter-quartile range). You'll see this a bit more later when we look at the Iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more plotting: box-and-whisker\n",
    "\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = plt.axes()\n",
    "ax.boxplot(ages, vert=False)\n",
    "ax.grid()\n",
    "ax.set_xlabel(r\"ages\")\n",
    "ax.text(2.5, 0.8, \"LQ\", horizontalalignment='center')\n",
    "ax.text(5, 0.8, \"median\", horizontalalignment='center', color=\"C1\")\n",
    "ax.text(7.5, 0.8, \"UQ\", horizontalalignment='center')\n",
    "ax.text(0, 0.8, \"LR\", horizontalalignment='center')\n",
    "ax.text(10, 0.8, \"UR\", horizontalalignment='center')\n",
    "\n",
    "ax.annotate(\"\", xy=(7.5, 1.2), xytext=(2.5, 1.2), arrowprops=dict(arrowstyle=\"<->\", color=\"C2\"))\n",
    "ax.text(5, 1.25, \"IQR\", horizontalalignment='center', color=\"C2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and variance / standard deviation\n",
    "\n",
    "For $N$ samples with outcomes $x_i$, the **mean** of a sample $\\overline{x}$ is defined as\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\overline{x} = \\frac{x_1 + x_2 + \\ldots + x_N}{N} = \\frac{1}{N}\\sum_{i=1}^N x_i,\n",
    "\\end{equation*}\n",
    "\n",
    "i.e. sum up all numbers (unweighted) and divide by the total number of samples.\n",
    "\n",
    "> NOTE: I am going to be making a distinction in *05/06_statistical_tests* between the **sample mean** $\\overline{x}$ and the **population mean** $\\mu$ (the Greek character \"mu\", i.e. like the sound a cat makes).\n",
    "\n",
    "So you need a way to count the number of samples, and to sum things. The code below shows how you might do this in a brute force way, a slightly more elegant way, and the easy way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the really old fashioned way (partly to demonstrate some more syntax)\n",
    "# compute the sum and count the number of numbers, then compute the mean\n",
    "ages_avg, count = 0.0, 0\n",
    "for i in range(len(ages)):\n",
    "    count    += 1   # \"count += 1\" is equivalent to \"count = count + 1\"\n",
    "    ages_avg += ages[i]\n",
    "\n",
    "print(f\"mean from the slow way = {ages_avg / count}\")\n",
    "\n",
    "# the cleaner way\n",
    "ages_sum = np.sum(ages)\n",
    "count = len(ages)\n",
    "print(f\"mean from sum / count  = {ages_sum / count}\")\n",
    "\n",
    "# the inbuilt way\n",
    "print(f\"mean from using np.mean= {np.mean(ages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, I am going use the unadjusted version of the **variance** $s^2$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    s^2 = \\frac{(x_1 - \\overline{x})^2 + (x_2 - \\overline{x})^2 + \\ldots + (x_N - \\overline{x})^2}{N} = \\frac{1}{N}\\sum_{i=1}^{N} (x_i - \\overline{x})^2,\n",
    "\\end{equation*}\n",
    "\n",
    "i.e. take the sample mean from each sample away, square it, sum it, then divide by the number of samples. \n",
    "\n",
    "Two observations to makes here is that:\n",
    "\n",
    "1) The variance is positive definite, and is only zero if every sample is the same as the sample mean.\n",
    "\n",
    "2) If the sample size $N$ is large and only one sample is far from the mean, then that sample's contribution to the variance is rather minimal (because of the division by $N$).\n",
    "\n",
    "From this, convince yourself that the variance is a measure of spread, and deviation away from the mean. The (unadjusted) **standard deviation**, which I will be short handing as s.t.d., partly for the puns I can make, is just the square root of the variance.\n",
    "\n",
    "> NOTE: I am also going to make a distinction in *05/06_statistical_tests* between the **sample s.t.d.** $s$, and the **population s.t.d.** (heh) $\\sigma$\n",
    "\n",
    "The code below does the same as the calculations for the variance and the s.t.d., with one verbose way, one easier way, and one using the inbuilt command in `numpy` (`np.std`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate with only mean and variance for now\n",
    "\n",
    "ages = np.asarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=float)  # force this to be float to stop a complaint\n",
    "ages_avg = np.mean(ages)\n",
    "\n",
    "# take the mean off every number in the array, and then compute things needed for standard deviation\n",
    "# I am going to compute the variance then work out the s.d.\n",
    "ages_var, count = 0.0, 0  # reset the counter\n",
    "for i in range(len(ages)):\n",
    "    count    += 1\n",
    "    ages_var +=(ages[i] - ages_avg)**2\n",
    "\n",
    "ages_var /= count\n",
    "\n",
    "print(f\"stats from verbose code: mean = {ages_avg}, variance = {ages_var}, s.t.d. = {np.sqrt(ages_var):.3f}\")\n",
    "\n",
    "# slightly less verbose way of doing above\n",
    "ages_var = np.sum((ages - ages_avg)**2) / len(ages)\n",
    "print(f\"stats from cleaner code: mean = {ages_avg}, variance = {ages_var}, s.t.d. = {np.sqrt(ages_var):.3f}\")\n",
    "\n",
    "# using the inbuilt s.t.d.\n",
    "print(f\"stats from command     : mean = {np.mean(ages)}, variance = {np.var(ages)}, s.t.d. = {np.std(ages):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!! USER BEWARE !!!\n",
    "\n",
    "So here is an example why sometimes one should know the old-fashioned way of doing things, and be very careful when using inbuilt commands. Suppose I use instead another command for standard deviation from a different package (in this case the `statistics` package), and do the same thing above, I would get the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as stat\n",
    "\n",
    "print(f\"\"\"stats from another package: \n",
    "\n",
    "mean = {stat.mean(ages)}, variance = {stat.variance(ages)}, s.d. = {stat.stdev(ages):.3f}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean is the same but the variance and the s.t.d's (heh) are different, whats the deal?\n",
    "\n",
    "I was quite explicit in saying I was using the unadjusted variance / s.t.d., and there is an adjusted version with what's called the **Bessel correction**,\n",
    "\n",
    "\\begin{equation*}\n",
    "    s^2 = \\frac{(x_1 - \\overline{x})^2 + (x_2 - \\overline{x})^2 + \\ldots + (x_N - \\overline{x})^2}{N - 1} = \\frac{1}{N - 1}\\sum_{i=1}^{N} (x_i - \\overline{x})^2.\n",
    "\\end{equation*}\n",
    "\n",
    "The variance and s.t.d. calculations in `numpy` uses the unadjusted definition, while the `statistics` packages uses the adjusted version. If you blindly use packages you would probably get different answers (like some of the students did in ENVS 3004 at some point, when they tried to make it easier for themselves and not do it by hand, and thus got it wrong because I explicitly asked them to use the definition I provided).\n",
    "\n",
    "***Which should you use?*** I will be explicit in assignments. Personally I almost never deal with small samples or with statistical tests, so I tend to go with the unadjusted one (for large $N$, $N \\approx N - 1$ so it makes very little practical difference). If you deal with small samples you probably should use the adjusted version. You can still use `numpy`, but override the default in this case with the keyword `ddof` (look this up in the manual if you want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"stats from numpy with override (ddof=0 by default): \n",
    "\n",
    "mean = {np.mean(ages)}, variance = {np.var(ages, ddof=1)}, s.d. = {np.std(ages, ddof=1):.3f}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "# More involved exercises with this notebook\n",
    "\n",
    "## 1) Penguin data, stats and pandas\n",
    "\n",
    "The [Palmer Penguins](https://cran.r-project.org/web/packages/palmerpenguins/readme/README.html) data compiled as a replacement to the standard [iris data](https://en.wikipedia.org/wiki/Iris_flower_data_set) because of racism/eugenics reasons of Ronald Fisher (look it up if you are interested). A mildly touched up version is given here as `penguins.csv` (or https://raw.githubusercontent.com/julianmak/OCES3301_data_analysis/refs/heads/main/penguins.csv; I removed some columns and some `NaN`s). We are going to be using that dataset quite a bit up to *06_statistical_tests* to demonstrate code syntax.\n",
    "\n",
    "<img src=\"https://www.boredpanda.com/blog/wp-content/uploads/2020/08/cats-standing-like-penguins-fb-png__700.jpg\" width=\"500\" alt='cursed penguins'>\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Either download or open `penguins.csv` from the repository outside of python and see what it contains. The descriptors should be fairly self-explanatory actually.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Open the data in python. I would suggest using `pandas` (copy some of the code from above).\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Reproduce some of the statistics given by the `df.describe()` by hand, you might want some to use some of the codes below.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Do some standard plots or box plots of various attributes, remember to label your graphs.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Try and do a boxplot of say one attribute of the three different species of penguins together (Google will probably give you the syntax).\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> How would do you a scatter plot with `plt`? could you use `plt.plot` directly?\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (more involved) Provide some qualitative descriptions of the different attributes and its dependence on species. Note some of these down, as we will be quantifying these in the next four sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
