{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*updated 31 Dec 2024, Julian Mak (whatever with copyright, do what you want with this)\n",
    "\n",
    "### As part of material for OCES 3301 \"Data Analysis in Ocean Sciences\" delivered at HKUST\n",
    "\n",
    "For the latest version of the material, go to the public facing [GitHub](https://github.com/julianmak/academic-notes/tree/master/OCES3301_data_analysis_ocean) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some deafult packages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats, signal\n",
    "from scipy.ndimage import filters, gaussian_filter\n",
    "import pandas as pd\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "# 07: Times series\n",
    "\n",
    "Here the focus is on data with an associated time axis. Examples could include:\n",
    "\n",
    "* Temperature of somewhere over some period.\n",
    "\n",
    "* Whale calls (which has some pitch that varies in time).\n",
    "\n",
    "* Phytoplankton concentration over some patch of ocean.\n",
    "\n",
    "* Location of an eddy over time (this could be longitude/latitude as a function of time).\n",
    "\n",
    "* Cursedness of a being with exposure to camera over time.\n",
    "\n",
    "Time series analysis can serve as a whole course by itself (e.g. Jonathan Lilly's [time-series course](https://github.com/jonathanlilly/time-series)); see more examples there. We are only going to touch on a few tools here.\n",
    "\n",
    "> NOTE: A lot of things visited here work equally well when the *space* variable is of concern. See *09/10_fun_with_maps*.\n",
    "\n",
    "> NOTE: Through these two notebooks I am going to be using sines and cosines a lot, and by default when I am talking about the inputs of the sines and cosines I am going to be in units of radians. Most numerical programs expect arguments for trigonometric functions to be given in radians actually (Python is one of these). If you really insist on giving things in degrees, you could use the `np.deg2rad` function, which converts from degrees to radians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "# a) Smoothing\n",
    "\n",
    "In the case below I artificially created some time-series data (in this case 6-hourly) to manipulate. The below code just plots it (step -1 of data analysis: plot the data out and see what it looks like first). I am going to create a corresponding time array through the `np.datetime64` functionality.\n",
    "\n",
    "> NOTE: For `datetime64`, if you want to create things in smaller or bigger units (e.g. seconds, days) you might do `np.timedelta64(1, 's')` or `np.timedelta64(1, 'd')`. Look up online for related syntax.\n",
    "\n",
    "I'm not going to explain what the numbers are at the moment; see section d later for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vec = np.arange(np.datetime64('2020-01-01'), np.datetime64('2021-12-31'), np.timedelta64(6, 'h'))\n",
    "\n",
    "nt = len(time_vec)\n",
    "t_vec = np.linspace(0, 2.0 * np.pi, nt)\n",
    "lin_trend = 0.05 * np.linspace(0, 2.0 * np.pi, nt)\n",
    "\n",
    "noise = 0.2 * np.random.rand(nt)\n",
    "f_vec = (  2.7 \n",
    "         + 0.1 * np.sin(t_vec) \n",
    "         + 0.05 * np.sin(4.0 * t_vec) \n",
    "         + 0.02 * np.sin(60.0 * t_vec) \n",
    "         + lin_trend \n",
    "         + noise\n",
    "        )\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(time_vec, f_vec, 'C0-')\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"data\")\n",
    "ax.set_ylim([2.7, 3.2])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Remember this guy?)\n",
    "\n",
    "<img src=\"https://i.imgur.com/rRDngzC.jpg\" width=\"400\" alt='haku'>\n",
    "\n",
    "So I cooked up this time-series to have suggestions of features (e.g. longer time oscillations), but the graph is very dense with lots of points, which is potentially masking out a lot of the information. We could zoom in a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax = plt.subplot(3, 1, 1)\n",
    "ax.plot(time_vec, f_vec, 'C0-')\n",
    "# ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"data\")\n",
    "ax.set_ylim([2.7, 3.2])\n",
    "plt.axvspan(np.datetime64('2020-04'), np.datetime64('2020-09'), color='red', alpha=0.5)\n",
    "plt.axvspan(np.datetime64('2020-05'), np.datetime64('2020-06'), color='green', alpha=0.5)\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(3, 1, 2)\n",
    "ax.plot(time_vec, f_vec, 'C0-')\n",
    "# ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"data\")\n",
    "ax.set_ylim([2.7, 3.2])\n",
    "ax.set_xlim([np.datetime64('2020-04'), np.datetime64('2020-09')])\n",
    "plt.axvspan(np.datetime64('2020-05'), np.datetime64('2020-06'), color='green', alpha=0.5)\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(3, 1, 3)\n",
    "ax.plot(time_vec, f_vec, 'C0-')\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"data\")\n",
    "ax.set_ylim([2.7, 3.2])\n",
    "ax.set_xlim([np.datetime64('2020-05'), np.datetime64('2020-06')])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be some long term trend going up, as well as some oscillations at different **periods** (because I put it in afterall).\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (semi-theoretical) The code actually gives you enough information to tell you what the period is, so what are they in units of days/months say?\n",
    "\n",
    "There is however also noise (because I also put some in), and the task here is to try and pull that out the \"signals\" of interest in some way. One way we could do is to just brute force **downsize** the data, i.e., plot the data every so often (we will come back to this in *09_fun_with_maps*). The below code is an example of this; have a think what I actually did below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "ax.plot(time_vec, f_vec, 'C0-')\n",
    "# ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"data\")\n",
    "ax.set_ylim([2.7, 3.2])\n",
    "ax.set_title(\"original signal\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "ax.plot(time_vec[::20], f_vec[::20], 'C1-')  # downsizing is here: what am I doing?\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"data\")\n",
    "ax.set_ylim([2.7, 3.2])\n",
    "ax.set_title(\"downsized signal\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above procedure makes the plot less cluttered, but it doesn't really tell us much more that what we can see already, and there are still a lot of fluctuations going on. We may want to somehow **filter** those out.\n",
    "\n",
    "Formally, filtering a time-series $f(t)$ involves performing a **convolution** with a **kernel function** $G(t)$. The filtered time series I am denoting $f^{<}(t)$ is given by\n",
    "\n",
    "\\begin{equation*}\n",
    "    f^{<}(t) = (f\\ *\\ G)(t) = \\int f(\\tau)G(t - \\tau)\\; \\mathrm{d}\\tau,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\tau$ is a dummy time variable, and the integral is taken over the whole time of the signal. Here I am specifically thinking of a **low-pass filter**, where the filtering process keeps the lower frequencies (longer periods) and smooths out the higher frequencies (shorter periods); hence the notation $f^{<}(t)$, because we are keeping the lower frequencies.\n",
    "\n",
    "> NOTE: You can of course define a **high-pass filter** analogously, either through an appropriate definition of $G$, taking $f^{>} = f - f^{<}$, or something else. We are mostly going to focus on low-passes here.\n",
    "\n",
    "A perhaps illustrative example is that of **averaging over a moving time window**. I take some point in time $t_0$, consider a window around this point $(t_0 - T/2, t_0 + T/2)$, average all the values within the window, and call this the value of the filtered time-series. If we are assuming the data is constantly spaced in time $\\Delta t$, then we would have\n",
    "\n",
    "\\begin{equation*}\n",
    "    f^{<}(t_0) = \\frac{1}{T}\\int^{t_0 + T/2}_{t_0 - T/2} f(\\tau)\\; \\mathrm{d}\\tau \\approx \\frac{f_1 + f_2 + \\cdots + f_N}{N},\n",
    "\\end{equation*}\n",
    "\n",
    "where $f_1 = f(t_0 - T/2)$, $f_2 = f(t_0 - T/2 + \\Delta t)$, ... $f_{N-1} = f(t_0 + T/2 - \\Delta t)$, $f_N = f(t_0 + T/2)$, and we do this for every $t_0$ choice. \n",
    "\n",
    "> NOTE: The above turns into a simple average only because of the assumption of constant spacing in time. If that is not true in your data, you have to do a numerical integral (sometimes called a **quadrature**) for the data to be weighed properly.\n",
    ">\n",
    "> (To confuse you a bit, a quadrature happens to be a weighted sum that is an approximation to the integral. When we are dealing with numerical data, we are almost always doing sums and products.)\n",
    "\n",
    "Equivalently, this is setting the kernel to be\n",
    "\n",
    "\\begin{equation*}\n",
    "    G(\\tau) = \\begin{cases} 1/T & t - T/2 \\leq \\tau \\leq t + T/2, \\\\ 0 & \\mathrm{otherwise}\\end{cases}.\n",
    "\\end{equation*}\n",
    "\n",
    "See if the animation I made helps: the last value of the filtered signal in the top panel is obtained from averaging over the data in the green window.\n",
    "\n",
    "<img src=\"https://i.imgur.com/jD0HnE8.gif\" width=\"600\" alt='moving window'>\n",
    "\n",
    "The code below gives you the end result straight away, and overlays the original signal and the filtered signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 60                                             # specify a window (number of entries)\n",
    "half_window = int(window / 2)\n",
    "\n",
    "f_filter = []   # initialise lists and dump into them\n",
    "t_filter = []\n",
    "for i in range(half_window, len(f_vec) - half_window):\n",
    "    t_filter.append(time_vec[i])\n",
    "    f_filter.append(np.mean(f_vec[i-half_window:i+half_window]))\n",
    "    \n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.plot(time_vec, f_vec, 'C0-', label=\"original signal\")\n",
    "ax.plot(t_filter, f_filter, 'C1-', label=\"filtered signal\")\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"data\")\n",
    "ax.set_ylim([2.7, 3.2])\n",
    "ax.grid()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the averaging in this case removes the high-frequency (short period) fluctuations without down sampling; the data array size is comparable to the beginning. The signal has decreased in magnitude, because we are dealing with an averaging procedure, which by default removes the extremes in values. \n",
    "\n",
    "For the low-passed signal, we can perhaps see that there is a robust oscillation with a six month period, as well as hints of an oscillation over 24 months (less obvious because of the linear trend, because the data is identified over a time period of only 24 months anyway; convince yourself the oscillations I put in to this artificial time-series are exactly the ones I said above.\n",
    "   \n",
    "> NOTE: The averaging procedure I did chops off some end points. There are ways around this, though you might just need to be careful what packages actually do for the end points.\n",
    "   \n",
    "> <span style=\"color:red\">**Q.**</span> I chose `window = 60`, but what does this actually mean in terms of time units? Or, put another way, what is the averaging window (lets say in days) I am choosing? With this, convince yourself that the choice of `window = 60` severely damps one of the shorter period / higher frequencies I put in by hand when creating the signal.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Try playing around with the window size and see the effects.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (Needs a bit of care) Remove the low-passed signal from the original signal to get the analogous high-pass signal. Is the signal what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't have to use a uniform window. The below example uses a **tent kernel**. Instead of weighting all the values uniformly as above (through the use of `np.mean`), I could for example say I care more about the values near the $t_0$ location, and progressively care less as I move away, eventually going to zero. The below code is one demonstration of the tent kernel in action.\n",
    "\n",
    "> NOTE: The kernels that go to zero within some finite time in this setting would be an example of a kernel with **compact support**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 60                                             # specify a window (number of entries)\n",
    "half_window = int(window / 2)\n",
    "\n",
    "tent_kernel  = -np.abs(np.arange(window)+0.5 - window/2.0)   # make a straight line and bend it into a v\n",
    "tent_kernel -= np.min(tent_kernel)                           # flip the v upside down\n",
    "tent_kernel /= np.max(tent_kernel) / 2.0                     # normalise such that kernel integrates to window size\n",
    "                                                             #   so multiplcation then followed by np.mean leads\n",
    "                                                             #   to no damping as such\n",
    "\n",
    "f_filter2 = []   # initialise lists and dump into them\n",
    "t_filter = []\n",
    "for i in range(half_window, len(f_vec) - half_window):\n",
    "    t_filter.append(time_vec[i])\n",
    "    f_filter2.append(np.mean(tent_kernel * f_vec[i-half_window:i+half_window]))\n",
    "    \n",
    "fig = plt.figure(figsize=(14, 4))\n",
    "ax = plt.subplot2grid((1, 3), (0, 0), colspan=2)\n",
    "ax.plot(time_vec, f_vec, 'C0-', label=\"original signal\")\n",
    "ax.plot(t_filter, f_filter,  'C1-', label=\"filtered signal: uniform\")\n",
    "ax.plot(t_filter, f_filter2, 'C3-', label=\"filtered signal: tent\")\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"data\")\n",
    "ax.set_ylim([2.7, 3.2])\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "ax = plt.subplot2grid((1, 3), (0, 2), colspan=1)\n",
    "ax.plot(np.ones(len(tent_kernel)), 'C1-', label=\"uniform kernel\")\n",
    "ax.plot(tent_kernel, 'C3-', label=\"tent kernel\")\n",
    "ax.set_xlabel(r\"index\")\n",
    "ax.set_ylabel(r\"kernel shape\")\n",
    "ax.set_ylim([0.0, 2.0])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtered signal resulting from the tent kernel visually keeps a bit more of the fluctuations; this could be quantified more in the frequency domain (see *08_time_series*).\n",
    "\n",
    "Another popular filter to use is the **Gaussian filter** and, you might have guessed from the name, the kernel function is the Gaussian function, which you have seen in *05_statistical_tests*. Instead of a window, a Gaussian filter width is related some specified standard deviation. The Gaussian kernel formally is **non-compact**, although in practice there is usually a default chop off of the kernel after a certain range, because otherwise the convolution procedure (which is an integral, so numerically these are sums) becomes computational expensive to carry out.\n",
    "\n",
    "The below is an example of a Gaussian filter through `scipy.ndimage.filters` (not going to bother coding this from scratch, but you could if you want some Python practice...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default is 1d Gaussian (reshape data into n-d to do n-d filtering)\n",
    "\n",
    "sigma = 2.0  # here the value to specify is sigma, which determines kernel shape\n",
    "f_vec_gauss_avg = gaussian_filter(f_vec, sigma)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "ax.plot(time_vec, f_vec, 'C0-', label=\"original signal\")\n",
    "ax.plot(time_vec, f_vec_gauss_avg, 'C1-', label=r\"$\\sigma = %.1f$\" % sigma)\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"data\")\n",
    "ax.set_ylim([2.7, 3.2])\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "sigma = 10.0\n",
    "f_vec_gauss_avg = gaussian_filter(f_vec, sigma)\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "ax.plot(time_vec, f_vec, 'C0-', label=\"original signal\")\n",
    "ax.plot(time_vec, f_vec_gauss_avg, 'C1-', label=r\"$\\sigma = %.1f$\" % sigma)\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"data\")\n",
    "ax.set_ylim([2.7, 3.2])\n",
    "ax.grid()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> Why is the latter result more smooth?\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Why does this not need `time_vec` to be reduced in size? Look up what the Gaussian filter default is at the edges of the data array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above are just some examples of smoothing procedures, and are by no means exhaustive (e.g. **wavelets**, see exercise here). We will revisit aspects of this part again once we talked a bit about the frequency domain and the spectrum in *08_time_series*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "# b) Trends\n",
    "\n",
    "Generically this would mean the **linear trend**, which is the slope of the linear regression of the data (i.e. $a$ in the regressed model $y = f(t) = at + b$). The slope/trend $a$ tells you how $y$ changes with every factor of $t$. For example, if I had\n",
    "\n",
    "\\begin{equation*}\n",
    "    {\\rm cursedness} = 0.7\\ ({\\rm length\\ of\\ camera\\ exposure}) + 1.0,\n",
    "\\end{equation*}\n",
    "\n",
    "and I measure cursedness in units of micro-cathulu ($\\mu{\\rm Cat} = 10^{-6}\\ {\\rm Cat}$) and length of camera exposure in the SI unit seconds (${\\rm s}$), then my (linear) trend here would be $0.7 \\mu{\\rm Cat}\\ \\mathrm{s}^{-1}$.\n",
    "\n",
    "<img src=\"https://i.imgur.com/OPae3B2.jpg\" width=\"600\" alt='100 micro-cat and a non-cursed cat'>\n",
    "\n",
    "We could look at this for the artificial time-series as well as its low-passed version above. In this case we know exactly what the trend is supposed to be, because we made the time-series (what is it?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-create the time-series for easier referencing\n",
    "\n",
    "nt = len(time_vec)\n",
    "t_vec = np.linspace(0, 2.0 * np.pi, nt)\n",
    "lin_trend = 0.05 * np.linspace(0, 2.0 * np.pi, nt)\n",
    "\n",
    "noise = 0.2 * np.random.rand(nt)\n",
    "f_vec = (  2.7 \n",
    "         + 0.1 * np.sin(t_vec) \n",
    "         + 0.05 * np.sin(4.0 * t_vec) \n",
    "         + 0.02 * np.sin(60.0 * t_vec) \n",
    "         + lin_trend \n",
    "         + noise\n",
    "        )\n",
    "\n",
    "f_vec_gauss_avg = gaussian_filter(f_vec, 10)\n",
    "\n",
    "p_orig  = np.polyfit(t_vec, f_vec, 1)\n",
    "p_gauss = np.polyfit(t_vec, f_vec_gauss_avg, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "ax.plot(time_vec, f_vec, label=r\"original signal\")\n",
    "ax.plot(time_vec, np.polyval(p_orig, t_vec), 'k--',  # regressed linear trend\n",
    "        label=f\"${{{p_orig[0]:.3f}}} t + {{{p_orig[1]:.3f}}}$\")  \n",
    "ax.set_ylabel(r\"SST (${}^\\circ\\mathrm{C}$)\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "ax.plot(time_vec, f_vec_gauss_avg, \"C1-\", label=r\"Gaussian filtered\")\n",
    "ax.plot(time_vec, np.polyval(p_gauss, t_vec), 'k--',  # regressed linear trend\n",
    "        label=f\"${{{p_gauss[0]:.3f}}} t + {{{p_gauss[1]:.3f}}}$\")  \n",
    "ax.set_xlabel(r\"$t$ (years)\")\n",
    "ax.set_ylabel(r\"SST (${}^\\circ\\mathrm{C}$)\")\n",
    "ax.grid()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is reassuring here that the filtering ends up preserving the trends, because that is not an obvious property of the filter (or is it?)\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (Mathematical) Is it provable property that filters preserve the linear trends? Try and search the internet and see if there is any info on this. You could also try some numerical experiments, though that itself would not be a proof.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> The second coefficient (the constant) should be `2.7` because that's what I used to create the time-series, however the linear regression does not give that. Is this a problem? Why?\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> The slope I gave the linear trend was `0.05`, but the value from the regression is not that. The answer is actually correct, but why? (Hint: what is time here?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**De-trending** in this case just means removing the (linear) trend from the signal, which is done below (notice the difference in the axes values). The resulting signal is sometimes called the **anomalies** (relative to the linear trend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = plt.axes()\n",
    "ax.plot(time_vec, f_vec - np.polyval(p_orig, t_vec), label=r\"detrended original signal\")\n",
    "ax.plot(time_vec, f_vec_gauss_avg - np.polyval(p_gauss, t_vec), \"C1-\", label=r\"detrended Gaussian filtered signal\")\n",
    "ax.set_xlabel(r\"$t$ (years)\")\n",
    "ax.set_ylabel(r\"SST (${}^\\circ\\mathrm{C}$)\")\n",
    "ax.grid()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is of course the possibility of obtaining and/or removing the quadratic trends etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "# c) Correlations\n",
    "\n",
    "Just like we could ask about the correlation between two datasets (e.g. *03_regression*), we could ask how two signals are correlated in (almost) exactly the same way. Below I am going to use artificial data to make a few points in cases where where we should basically know the answer before we even ask the computer to do stuff for us. The lines we are plotting here are (for $0 \\leq t \\leq 2\\pi$)\n",
    "\n",
    "\\begin{equation*}\n",
    "    f(t) = \\sin(t), \\qquad f^+ = 2 \\sin(t), \\qquad f^- = -\\sin(t), \\qquad f_{\\rm shift} = \\sin\\left(t - \\frac{\\pi}{2}\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> What correlations do you expect the signals are relative to $f$? Try and give your answer without running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_vec   = np.linspace(0, 2.0 * np.pi, 31)\n",
    "f       =   np.sin(t_vec)\n",
    "f_pos   = 2*np.sin(t_vec)\n",
    "f_neg   =  -np.sin(t_vec)\n",
    "f_shift =   np.sin(t_vec - np.pi / 2.0)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = plt.axes()\n",
    "ax.plot(t_vec, f    ,   \"C0\", label=r\"$f$\")\n",
    "ax.plot(t_vec, f_pos,   \"C1\", label=r\"$f^+$\") \n",
    "ax.plot(t_vec, f_neg,   \"C2\", label=r\"$f^-$\")\n",
    "ax.plot(t_vec, f_shift, \"C3\", label=r\"$f_{\\rm shift}$\")\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$f$\")\n",
    "ax.grid()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be clearer if we plot the various signals directly against $f$ in a scatter graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above but as scatter graph\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "ax1.scatter(f, f_pos, color=\"C1\")\n",
    "ax1.set_xlabel(r\"$f$\")\n",
    "ax1.set_ylabel(r\"$f^+$\")\n",
    "ax1.grid()\n",
    "\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "ax2.scatter(f, f_neg, color=\"C2\")\n",
    "ax2.set_xlabel(r\"$f$\")\n",
    "ax2.set_ylabel(r\"$f^-$\")\n",
    "ax2.grid()\n",
    "\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "ax3.scatter(f, f_shift, color=\"C3\")\n",
    "ax3.set_xlabel(r\"$f$\")\n",
    "ax3.set_ylabel(r\"$f_{\\rm shift}$\")\n",
    "ax3.grid()\n",
    "\n",
    "fig.tight_layout(pad=1.0) # give the graph a bit of padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know what the answer you should be getting, lets see if the direct computations will be reproducing those. I am going to be using the `scipy.stats` package for this, but you could use `scikit-learn` or even brute force computing the (Pearson/linear) correlation coefficient if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, r_pos, _, _ = stats.linregress(f, f_pos)\n",
    "_, _, r_neg, _, _ = stats.linregress(f, f_neg)\n",
    "_, _, r_shift, _, _ = stats.linregress(f, f_shift)\n",
    "\n",
    "print(f\"f and f_pos   has (linear/Pearson) correlation coefficient of {r_pos:.2f}\")\n",
    "print(f\"f and f_neg   has (linear/Pearson) correlation coefficient of {r_neg:.2f}\")\n",
    "print(f\"f and f_shift has (linear/Pearson) correlation coefficient of {r_shift:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross correlation\n",
    "\n",
    "Going to focus on the shifted signal above. Below is the same example but extended in the $t$ variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_vec   = np.linspace(0, 4.0 * np.pi, 61)\n",
    "f       =   np.sin(t_vec)\n",
    "f_shift =   np.sin(t_vec - np.pi / 2.0)\n",
    "\n",
    "_, _, r_shift, _, _ = stats.linregress(f, f_shift)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(t_vec, f    ,   \"C0\", label=r\"$f$\")\n",
    "ax.plot(t_vec, f_shift, \"C3\", label=r\"$f_{\\rm shift}$\")\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$f$\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(f\"$r = {r_shift:.2f}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By construction there is no linear correlation between the signals, but doing the all important step -1 of data analysis (plotting the data out and just looking at it) will tell you there is clearly some correlation between the two, particularly if we choose to **lag** the signals. Below I am shifting the data and then computing the $r$-value again.\n",
    "\n",
    "> NOTE: In the way I am doing the shift you need to make sure the signal is of the same length, which means some chopping of the signals are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 5  # lag by 5 indices\n",
    "\n",
    "_, _, r_lag, _, _ = stats.linregress(f[:-lag:], f_shift[lag::])  # chop the signal\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(t_vec[:-lag:], f      [:-lag:]    ,   \"C0\", label=r\"$f$\")\n",
    "ax.plot(t_vec[:-lag:], f_shift[lag::], \"C3\", label=r\"$f_{\\rm shift}$ with lag\")\n",
    "ax.plot(t_vec, f_shift, \"C3--\", label=r\"$f_{\\rm shift}$ orig\", alpha=0.5)\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$f$\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(f\"lag = {lag}, correlation coefficient of {r_lag:.2f}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done the mantra of \"trying small before going big\", the below subroutine performs a **cross correlation analysis** (which is a product from the **lagged regressions**) of the two signals.\n",
    "\n",
    "> NOTE: I have included an example of a **error catching** in the subroutine below that may or may not be of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_lag_corr(signal1, signal2, lag):\n",
    "    if len(signal1) != len(signal2):\n",
    "        raise Exception(\"array size not equal, cannot continue\")\n",
    "\n",
    "    if lag == 0:\n",
    "        _, _, r, _, _ = stats.linregress(signal1, signal2)\n",
    "    else:\n",
    "        _, _, r, _, _ = stats.linregress(signal1[:-lag:], signal2[lag::])\n",
    "    \n",
    "    return r\n",
    "\n",
    "n = 30\n",
    "r_lag = np.zeros(n)\n",
    "for lag in range(n):\n",
    "    r_lag[lag] = custom_lag_corr(f, f_shift, lag)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(np.arange(n), r_lag, \"C0-x\")\n",
    "ax.set_xlabel(r\"lag (in index)\")\n",
    "ax.set_ylabel(r\"$r$\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if the value of $n$ (the extent of the lag) gets too big, the number of samples to calculate the correlation becomes small, and it would increasingly become a dodgy manoeuvre (hence why I extended the signal by another period). In the present case this is not really an issue because the signal is very clean, but is often not the case with a \"real\" signal.\n",
    "\n",
    "In the above the lag is given per `index`, although we might be interested in what that corresponds to in \"real\" time. What needs to be done here is finding out what `lag = 1` actually means (it's really just a change of units). In this case I can get that from the time array by taking differences of the time vector, and since I also know the time array is uniform in time, I only need one of the entries, so might as well pick the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = np.diff(t_vec)[0]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(np.arange(n) * dt, r_lag, \"C0-x\")\n",
    "ax.plot([np.pi / 2, np.pi / 2], [-2, 2], \"k--\", alpha=0.7)          # theoretical maximum\n",
    "ax.plot([3 * np.pi / 2, 3 * np.pi / 2], [-2, 2], \"k--\", alpha=0.7)  # theoreical minimum\n",
    "ax.set_xlabel(r\"lag (in time units)\")\n",
    "ax.set_ylabel(r\"$r$\")\n",
    "ax.set_ylim([-1.1, 1.1])\n",
    "ax.grid()\n",
    "\n",
    "# add the tick labels in\n",
    "xt = ax.get_xticks() \n",
    "xt = np.append(xt, [np.pi/2, 3*np.pi/2])\n",
    "xtl= xt.tolist()\n",
    "xtl[-2]=r\"$\\pi/2$\"\n",
    "xtl[-1]=r\"$3\\pi/2$\"\n",
    "ax.set_xticks(xt)\n",
    "ax.set_xticklabels(xtl)\n",
    "ax.set_xlim([0, 6]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> Are the answers here consistent and what you expected?\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> There is `np.correlate` that could do something similar. Look up the syntax of that function and try using that too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-correlation\n",
    "\n",
    "You could choose two different signals, or the same signal twice, appropriately shifted. A cross-correlation analysis like the above for the same signal is called an **auto-correlation** (\"auto\" $\\leftrightarrow$ \"self\"), and is a measure of how related the signal is to lagged versions of itself. The below is one an example of the **auto-correlation function (ACF)**, using the sine curve example just now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trivial example\n",
    "\n",
    "t_vec = np.linspace(0, 2.0 * np.pi, 31)\n",
    "f     = np.sin(t_vec)\n",
    "dt = np.diff(t_vec)[0]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 3))\n",
    "\n",
    "n = 15\n",
    "r_lag = np.zeros(n)\n",
    "for lag in range(n):\n",
    "    r_lag[lag] = custom_lag_corr(f, f, lag)\n",
    "    \n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(np.arange(n), r_lag, \"C0-x\")\n",
    "ax.set_xlabel(r\"lag (in index)\")\n",
    "ax.set_ylabel(r\"ACF\")\n",
    "ax.set_title(f\"lag up to index {n}\")\n",
    "ax.grid()\n",
    "\n",
    "n = 30\n",
    "r_lag = np.zeros(n)\n",
    "for lag in range(n):\n",
    "    r_lag[lag] = custom_lag_corr(f, f, lag)\n",
    "    \n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(np.arange(n), r_lag, \"C0-x\")\n",
    "ax.set_xlabel(r\"lag (in index)\")\n",
    "ax.set_ylabel(r\"ACF\")\n",
    "ax.set_title(f\"lag up to index {n}\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> Are the answers here consistent and what you expected? Maybe plot out the actual signal and the lags to convince yourself the code is doing what it is supposed to do.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> This is a sine/cosine curve and the lag correlations should be symmetric, but the right panel is not symmetric about the minimum point, why? (Hint: what is the size of array and what is the lag?)\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> With the above answer, think of a way (or several ways) to get rid of that above offending reason and show that one can get a symmetric auto-correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oscillations in the ACF would be indicative of periodic behaviour. ACF also gives you a measure of how predictable the signal is: if the ACF is high for a while, then knowing the value at one point gives you an indication several points ahead, whilst a fast drop indicates the signal decorrelates with itself and loses predictability quickly.\n",
    "\n",
    "The ACF I did above is just a lag-correlation, where the correlation is normalised by the *sample* mean and variances, which changes as I change the magnitude of the lag, because I end up chopping bits of the signal out as I do the lag correlation. There are packages that have the ACF built in (e.g. `statsmodel` package), but in some cases the ACF is normalised by the *population* mean and variance of the whole, or have some other properties in there, so you might just need to be careful (it's always good to know multiple ways of doing the same thing).\n",
    "\n",
    "Below is calculating the ACF through `pandas`, which uses `np.correlate`, and is done exactly like the way I did it above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(f)\n",
    "n = 30\n",
    "acf = np.zeros(n)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 3))\n",
    "\n",
    "n = 15\n",
    "acf = np.zeros(n)\n",
    "for lag in range(n):\n",
    "    acf[lag] = s.autocorr(lag=lag)\n",
    "    \n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(np.arange(n), acf, \"C0-x\")\n",
    "ax.set_xlabel(r\"lag (in index)\")\n",
    "ax.set_ylabel(r\"ACF\")\n",
    "ax.set_title(f\"lag up to index {n}\")\n",
    "ax.grid()\n",
    "\n",
    "n = 30\n",
    "acf = np.zeros(n)\n",
    "for lag in range(n):\n",
    "    acf[lag] = s.autocorr(lag=lag)\n",
    "    \n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(np.arange(n), acf, \"C0-x\")\n",
    "ax.set_xlabel(r\"lag (in index)\")\n",
    "ax.set_ylabel(r\"ACF\")\n",
    "ax.set_title(f\"lag up to index {n}\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: If you use the `acf` function in `statsmodel` you will find the ACF is calculated differently and by default decays with lag. It also provides things like confidence intervals and other fancy things. If you are interested, try and follow the tutorial [here](https://www.alpharithms.com/autocorrelation-time-series-python-432909/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "# d) Power spectrum\n",
    "\n",
    "Returning to the oscillatory data, recall we did filtering to remove some of the oscillations. One thing we might want to do is in fact quanitify how much a signal is oscillating at a particular periods/frequency. We will use the artificial example above to demonstrate what we mean (partly because we know exactly what we threw in, so we already know the answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same cooked up signal as above at a different frequency\n",
    "# note each index here corresponds to **2 hours** to make it slightly higher frequency\n",
    "\n",
    "time_vec = np.arange(np.datetime64('2020-01-01'), np.datetime64('2021-12-31'), np.timedelta64(2, 'h'))\n",
    "\n",
    "nt = len(time_vec)\n",
    "t_vec = np.linspace(0, 2.0 * np.pi, nt)\n",
    "lin_trend = 0.05 * np.linspace(0, 2.0 * np.pi, nt)\n",
    "\n",
    "noise = 0.2 * np.random.rand(nt)\n",
    "f_vec = (  2.7 \n",
    "         + 0.1 * np.sin(t_vec) \n",
    "         + 0.05 * np.sin(4.0 * t_vec) \n",
    "         + 0.02 * np.sin(60.0 * t_vec) \n",
    "         + lin_trend \n",
    "         + noise\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In more detail, the above signal is created over **two years** in units of **2 hours**, with several components:\n",
    "\n",
    "* a constant part (the `2.7`)\n",
    "* an oscillation described by `sin` with a \"frequency\" of `1.0`\n",
    "* a smaller oscillation with a \"frequency\" of `4.0`\n",
    "* an even smaller oscillation with a \"frequency\" of `60.0`\n",
    "* a linear trend that has no oscillation\n",
    "* a noise part that is doing random things\n",
    "\n",
    "What we are aiming to pick out are those things described by sines (or cosines if we had it, which are just sines but shifted a bit), and notice I made these with a decreasing amplitude as I increase the frequency.\n",
    "\n",
    "The thing we want to compute is what is called a **power spectral density**, denoted\n",
    "\\begin{equation*}\n",
    "          PSD_f(\\omega) = \\mathcal{F}[\\mathcal{A}_f(t)] = |\\mathcal{F}[f(t)]|^2,\n",
    "\\end{equation*}\n",
    "where $\\mathcal{F}$ is the **Fourier transform**, $\\mathcal{A}_f(t)$ is the autocorrelation function of $f$ (and is a function of time), and $\\omega=2\\pi \\nu$ is the **angular freqency** (in units of cycles per time), while $\\nu$ is the frequency (in units of per time, usually measured in Hertz [Hz]). The computation of $\\mathcal{A}$ can be done done through a convolution (e.g. `np.convolve`), but it is faster to do it through a Fourier transform (so we would have to talk about what a Fourier transform is anyway...)\n",
    "\n",
    "We are going to focus on intrepreting the resulting graphs, because otherwise we would need to go through the theory of Fourier transforms, which takes quite a bit of time.\n",
    "\n",
    "> NOTE: For those interested, have a look at the old `08_times_series` notebook. Fourier transforms, its associated spectral representation and the computation of the Fourier transform is one of the things that allows the modern world to function the way it is (it's importance signified by the fact that almost nobody talks about it...)\n",
    "\n",
    "The routine to use is `scipy.signal.periodogram(f_vec, fs=1.0)` (ignoring other options in the routine). `f_vec` is the input signal, and `fs` is the sampling rate. We will do it *wrong* for the time being and just use the default `fs`, and I am *not* going to use the `freq` variable that comes with the routine for the time being (because it's not in useful units for reasons to be elaborated on). I am also going to mark on the $1$, $4$ and $60$ vertical lines, to correspond to the \"frequencies\" of the oscillations I put into the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a power spectrum density and associated frequencies, and do a plot (linear and log)\n",
    "\n",
    "freq, psd = signal.periodogram(f_vec)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 3))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(psd)\n",
    "ax.plot([1, 1], [0, 100], 'k--', alpha=0.3)\n",
    "ax.plot([4, 4], [0, 100], 'k--', alpha=0.3)\n",
    "ax.plot([60, 60], [0, 100], 'k--', alpha=0.3)\n",
    "ax.set_xlabel(\"freq\")\n",
    "ax.set_ylabel(\"PSD\")\n",
    "ax.set_xlim([0, 100])\n",
    "ax.set_ylim([0, 15])\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.loglog(psd)\n",
    "ax.loglog([1, 1], [0, 100], 'k--', alpha=0.3)\n",
    "ax.loglog([4, 4], [0, 100], 'k--', alpha=0.3)\n",
    "ax.loglog([60, 60], [0, 100], 'k--', alpha=0.3)\n",
    "ax.set_ylim([1e-8, 1e2])\n",
    "ax.set_xlabel(\"freq\")\n",
    "ax.set_ylabel(\"PSD\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So some comments both good and bad/unexpected:\n",
    "\n",
    "* There is a peak in the PSD corresponding to `60` which is very visible (more so in graph using logscales). A distinct peak in the PSD then corresponds to an oscillation at the corresponding frequency.\n",
    "* There is a peak at smaller frequencies, but these don't quite correspond to `1` or `4`?\n",
    "\n",
    "There are several reasons for this, the most notable one being the presence of non-oscillatory terms. What we want is to get rid of the non-oscilltory terms. \n",
    "\n",
    "> NOTE: The above reason is \"obvious\" if we talked about Fourier transforms and spectral representations, but just take my word for it if not. These non-oscillatory terms essentially all get dumped into the zero frequency case.\n",
    "\n",
    "In this case we have the original signal so we could have just commented out the things that clearly do not contribute to an oscillation (the linear and the constant term; we can't say anything about the noise because it's random, so for all you know it could have randomly got some oscillations at isolated moments in time). \n",
    "\n",
    "Another is to **detrend** the data. From visually inspecting the data it looks like it has a linear trend, so we can try removing that first: do a linear regression on the data, then remove the data relative to that regressed result. Below code does that and then generates the PSD plots again, using `scipy.signal.detrend`. I am additionally going to get rid of the `0` entry in `psd` because `0` doesn't correspond to oscillations anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detrend then do PSD again\n",
    "\n",
    "f_mod = signal.detrend(f_vec, type=\"linear\")\n",
    "\n",
    "freq, psd = signal.periodogram(f_mod)\n",
    "psd[0] = 0.0\n",
    "\n",
    "fig = plt.figure(figsize=(14, 3))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(psd)\n",
    "ax.plot([1, 1], [0, 100], 'k--', alpha=0.3)\n",
    "ax.plot([4, 4], [0, 100], 'k--', alpha=0.3)\n",
    "ax.plot([60, 60], [0, 100], 'k--', alpha=0.3)\n",
    "ax.set_xlabel(\"freq\")\n",
    "ax.set_ylabel(\"PSD\")\n",
    "ax.set_xlim([0, 100])\n",
    "ax.set_ylim([0, 15])\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.loglog(psd)\n",
    "ax.loglog([1, 1], [0, 100], 'k--', alpha=0.3)\n",
    "ax.loglog([4, 4], [0, 100], 'k--', alpha=0.3)\n",
    "ax.loglog([60, 60], [0, 100], 'k--', alpha=0.3)\n",
    "ax.set_ylim([1e-8, 1e2])\n",
    "ax.set_xlabel(\"freq\")\n",
    "ax.set_ylabel(\"PSD\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the `4` coming out, although the `1` is not. Part of the reason is that the `1` signal is too close to `0` and the computation just doesn't come out very well it turns out (it is what it is as in most cases with realistic data).\n",
    "\n",
    "Now we want to try and do this \"properly\" and give the frequency in useful units. If you simply to e.g. `ax.plot(freq, psd)` you will find the frequencies seem to have no correspondence to things you have. The two reasons are\n",
    "\n",
    "* I haven't specified the sampling rate `fs` properly yet, which is in units of 1/time\n",
    "* I am also working in weird units of time at the moment\n",
    "\n",
    "Fixing the second should fix the first also. Now this bit gets bit convoluted (but it really is just change of units). Taking the frequency `1` case as an example, note that (easy but less robust explanation first):\n",
    "\n",
    "* Because I took my code time length to be from $t=0$ to $t=2\\pi$, $f=1$ here corresponds to 1 oscillation over the whole period. The whole period I created to mean $2$ years, so $f=1$ corresponds to a period of $2$ years. So this arises because I was quite inconsistent in choosing units for time when I made the original notebook a few years ago (I got confused as to what was going on...)\n",
    "\n",
    "* Alternatively, I am really measuring my time in units of 2 years (!!!), and what I really have something is an oscillation described by $\\sin(2\\pi f t)$, where $f$ is my frequency. Frequency has units of $1 / \\mathrm{time} = 1 / 2 \\mathrm{years}$ in this case. $2\\pi$ means 1 cycle, so $\\omega = 2\\pi f$ is in units of $\\mathrm{cycles} / 2 \\mathrm{years}$, so above really corresponds to one cycle every two years.\n",
    "\n",
    "The main upshot is that I should have oscillations at 2 years, half a year (because $2\\ \\mathrm{years} = 4 \\times 0.5\\ \\mathrm{years}$), and roughly every $1/30$ year. In days, this is roughly 720, 180 and 12 days.\n",
    "\n",
    "If I decide to my units in \"years\", since I created the above signal with a spacing of 2 hours, and we have $1\\ \\mathrm{hour} = 1 / 24\\ \\mathrm{day} \\approx 1 / 24 / 360\\ \\mathrm{year}$. Thus $2\\ \\mathrm{hours} = (2 / 8640)\\ \\mathrm{years}$, but the routine wants `fs` as a ***sampling rate*** (which has units of per time), so we want to flip that upside down, i.e. sampling rate is $8640 / 2 = 4320\\ \\mathrm{year}^{-1}$. The corresponding frequencies of the oscillations we put in should thus be $1/2 = 0.5$, $1/0.5 = 2.0$ and $1/(1/30) = 30.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detrend then do PSD again\n",
    "\n",
    "f_mod = signal.detrend(f_vec, type=\"linear\")\n",
    "\n",
    "freq, psd = signal.periodogram(f_mod, fs=4320)\n",
    "psd[0] = 0.0\n",
    "\n",
    "fig = plt.figure(figsize=(14, 3))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(freq, psd)\n",
    "ax.plot([0.5, 0.5], [0, 100], 'k--', alpha=0.3)\n",
    "ax.plot([2.0, 2.0], [0, 100], 'k--', alpha=0.3)\n",
    "ax.plot([30.0, 30.0], [0, 100], 'k--', alpha=0.3)\n",
    "ax.set_xlabel(\"freq ($\\mathrm{yr}^{-1}$)\")\n",
    "ax.set_ylabel(\"PSD\")\n",
    "ax.set_xlim([0, 60])\n",
    "ax.set_ylim([0, 2e-3])\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.loglog(freq, psd)\n",
    "ax.loglog([0.5, 0.5], [0, 100], 'k--', alpha=0.3)\n",
    "ax.loglog([2.0, 2.0], [0, 100], 'k--', alpha=0.3)\n",
    "ax.loglog([30.0, 30.0], [0, 100], 'k--', alpha=0.3)\n",
    "ax.set_ylim([1e-10, 1e-2])\n",
    "ax.set_xlabel(r\"freq ($\\mathrm{yr}^{-1}$)\")\n",
    "ax.set_ylabel(\"PSD\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the graph limits have changed, which is related to the normalisation factor (which depends on the sampling rate in this routine); here we don't care particularly because we are only looking for peaks, and the normalisation shifts everything up or down. \n",
    "\n",
    "Below does this when our time units are in \"days\". Convince yourself I should have `fs = 12` and my peaks should be located at $1/720$, $1/180$ and $1/12$.\n",
    "\n",
    "> NOTE: The slight offsets in the peaks are probably because I took 360 days to be a year for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detrend then do PSD again\n",
    "\n",
    "f_mod = signal.detrend(f_vec, type=\"linear\")\n",
    "\n",
    "freq, psd = signal.periodogram(f_mod, fs=12)\n",
    "psd[0] = 0.0\n",
    "\n",
    "fig = plt.figure(figsize=(14, 3))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(freq, psd)\n",
    "ax.plot([1/720.0, 1/720.0], [0, 100], 'k--', alpha=0.3)\n",
    "ax.plot([1/180, 1/180.0], [0, 100], 'k--', alpha=0.3)\n",
    "ax.plot([1/12.0, 1/12.0], [0, 100], 'k--', alpha=0.3)\n",
    "ax.set_xlabel(\"freq ($\\mathrm{day}^{-1}$)\")\n",
    "ax.set_ylabel(\"PSD\")\n",
    "ax.set_xlim([0, 0.2])\n",
    "ax.set_ylim([0, 5e-1])\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.loglog(freq, psd)\n",
    "ax.plot([1/720.0, 1/720.0], [0, 100], 'k--', alpha=0.3)\n",
    "ax.plot([1/180, 1/180.0], [0, 100], 'k--', alpha=0.3)\n",
    "ax.plot([1/12.0, 1/12.0], [0, 100], 'k--', alpha=0.3)\n",
    "ax.set_ylim([1e-8, 1e1])\n",
    "ax.set_xlabel(r\"freq ($\\mathrm{day}^{-1}$)\")\n",
    "ax.set_ylabel(\"PSD\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> Compute the PSDs again but by modifying the original `f_vec` directly, commenting out all the non-oscillations and/or the noise, and see if that helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "# e) Application to El-Nino 3.4 data\n",
    "\n",
    "Just going to throw the tools we used above at the El-Nino 3.4 data. We read the data first. I am also going to make a raw number version of the time array instead of using datetime, because I want the raw numbers (e.g. to compute linear trends)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    f = open(\"elnino34_sst.data\", \"r\")\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES3301_data_analysis/refs/heads/main/elnino34_sst.data\"\n",
    "    f = urllib.request.urlopen(path)\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "elnino34_txt = f.readlines()\n",
    "elnino34_txt = elnino34_txt[3:-4]\n",
    "for k in range(len(elnino34_txt)):\n",
    "    if type(elnino34_txt[k]) == str:\n",
    "        elnino34_txt[k] = elnino34_txt[k].strip(\"\\n\")\n",
    "    elif type(elnino34_txt[k]) == bytes:\n",
    "        elnino34_txt[k] = elnino34_txt[k].decode(\"utf-8\").strip(\"\\n\")\n",
    "\n",
    "elnino34_txt[0].split()\n",
    "\n",
    "elnino34_sst = []\n",
    "for k in range(len(elnino34_txt)):           # this is the new elnino34_txt after stripping out some lines\n",
    "    dummy = elnino34_txt[k].split()          # split out the entries per line\n",
    "    for i in range(1, len(dummy)):           # cycle through the dummy list but skip the first entry\n",
    "        elnino34_sst.append(float(dummy[i])) # turn string into a float, then add to list\n",
    "\n",
    "elnino34_sst = np.array(elnino34_sst)\n",
    "\n",
    "# I want to do sums on this so I am going to use the raw version \n",
    "# (I personally find the numbers easier to manipulate)\n",
    "t_vec = np.linspace(1950, 2019+1, len(elnino34_sst), endpoint=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the time-series data below and compute the linear trend (through standard `np.polyfit`, but you can use something else). Note that the trend here is per **years**, because I made the time array in units of years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.polyfit(t_vec, elnino34_sst, 1)\n",
    "lin_trend = p[0] * t_vec + p[1]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(t_vec, elnino34_sst, 'C0')\n",
    "ax.plot(t_vec, lin_trend, 'k--')\n",
    "ax.text(1990, 24.5, f\"trend = ${p[0]:.3f}^{{\\circ}}\\ \\mathrm{{C}}$ per year\", color=\"k\")\n",
    "ax.set_xlabel(r\"$t$ (years)\")\n",
    "ax.set_ylabel(r\"SST (${}^{\\circ}\\mathrm{C}$)\")\n",
    "ax.set_ylim(24, 30)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> What does the trend mean here? Is this physically consistent with what is know? (You might need to look this up.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass the raw signal through a filter to smooth out some of the oscillations to help us pick out the longer oscillations. I am going to use a Gaussian filter with varying windows here, but you can try use something else (e.g. rolling average, a tent-like kernel, or otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3))\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.plot(t_vec, elnino34_sst, alpha=0.7)\n",
    "\n",
    "sigma_vec = [2.0, 5.0, 10.0]\n",
    "\n",
    "for sigma in sigma_vec:\n",
    "    elnino34_gauss = gaussian_filter(elnino34_sst, sigma)\n",
    "    ax.plot(t_vec, elnino34_gauss, label=f\"$\\sigma = {sigma}$\")\n",
    "ax.set_xlabel(r\"$t$ (years)\")\n",
    "ax.set_ylabel(r\"SST (${}^{\\circ}\\mathrm{C}$)\")\n",
    "ax.set_ylim(24, 30)\n",
    "ax.grid()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\sigma = 10$ case, while averaging quite hard and reducing the magnitudes, is really pulling out the longer oscillations, which by eye we can pick out to be around 2 to 7 years. We can quantify this better with another method in *08_time_series*.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Do a low pass of the signal for a specified window of 6 months, 2 years and 10 years, and describe signal, with both a not weighted and weighted (e.g. tent kernel) options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to calculate an ACF via the way I did it above. If you use the acf function from `statsmodel` you will get a qualitatively similar behaviour at the beginning, but something different beyond about 6 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = np.diff(t_vec)[0]\n",
    "\n",
    "f_vec = pd.Series(elnino34_sst)\n",
    "n = 12 * 10  # this would be 10 years\n",
    "\n",
    "acf = np.zeros(n)\n",
    "for lag in range(n):\n",
    "    acf[lag] = f_vec.autocorr(lag=lag)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = plt.axes()\n",
    "ax.plot(np.arange(n) * dt, acf)\n",
    "ax.set_ylim([-1, 1])\n",
    "ax.set_ylabel(r\"acf\")\n",
    "ax.set_xlabel(r\"lag (years)\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> How fast does the signal lose predictability in some sense?\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Normally we might expect once the signal loses predictability you might think that's it. However, there are oscillations persisting after that initial drop. What might be the physical cause of this?\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Investigate what happens if you compute the ACF like above *after* you low-pass the signal. Consider different sized windows.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (Harder and more involved) Here we used the whole time-series, but suppose you chop up the data in multiple chunks (say 20 year windows, shifted per year), and you repeat the acf analysis. Investigate if there are certain instance of periods of time (e.g. say during an El-Nino year) with a significantly different acf compared to the \"normal\", just by eye. Consider how you might do this statistically.\n",
    ">\n",
    "> (I don't precisely know what the acf in `statsmodel` does, but I have a suspicion if this is what it is doing. Given the lag, you chop up the regions and perform an analysis on all the sub-samples, and then compute the statistics of the acf. This is one reason I am not using it, because I don't really know what it is doing...)\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (Harder and more involved) As above, but consider say a 2 year period instead, and see if there changes in the \"predictability\" as suggested by the acf during El-Nino years. You will need to have a way to distinguish what constitutes an El-Nino year; usually this is done \"by above average temperatures\" over some threshold, i.e. large enough anomalies, but in this case you need the anomalies relative to the linear trend probably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the PSD, being careful with units of time. I have also done some other cosmetic things that I will not elaborate on; go through the lines of code to see what is going on, and convince yourself what I did below is sensible (I'm going to do this with the `f_vec` that is now a `pandas` series, to show in this case you can usually throw those into `numpy` and `scipy` routines as if they are the usual arrays).\n",
    "\n",
    "One main point that can be taken away from the graph below is that there is quite a lot of power in the 2 to 7 year range, indicating an oscillation that is predominantly over this range; this roughly corresponds to what we know about El-Nino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detrend then do PSD again\n",
    "\n",
    "freq, psd = signal.periodogram(f_vec, fs=12)\n",
    "psd[0] = 0.0\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.plot(1/freq, psd)\n",
    "ax.set_xlim([0, 10])\n",
    "ax.set_xticks(np.arange(10))\n",
    "ax.set_xlabel(r\"period ($\\mathrm{year}$)\")\n",
    "ax.set_ylabel(\"PSD\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Q.**</span> Why did I take `fs=12`?\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Why can I show things in this case in terms of \"periods\" instead of \"frequencies\", like in the PSDs I computed above?\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> You should get an error above about dividing by zero. What is going on? Even though it doesn't actually matter in this case, try and patch up the way the calculations are being done so there is no division by zero. (hint: related to question before)\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> There is a massive peak that I have basically ignored. What does it represent physically, and why might be justified that I just ignored it? (hint: what's the corresponding period?)\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (Slightly harder) What about the peak to the left of that massive peak I am referring to above? Why is it there? (hint: probably related to \"aliasing\"; argue whether this causes an issue for the higher multiples of the periods)\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Show the above PSD with period in units of months and/or days as a practice (I would make a copy of the cell so you don't modify what is already \"correct\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "# More involved exercises with this notebook\n",
    "\n",
    "## 1) Pandas\n",
    "\n",
    "Of course `pandas` have some of the functionality described above built in already, and remote loading is also builtin particularly for text files (in a less janky way than the one I did). Try look some of these up and reproduce some of the above within `pandas` (e.g., `.mean`, `.sum`, `.rolling`, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) El-Nino biogeochemistry data\n",
    "\n",
    "In `elnino34_bgc.data` is a processed reanalysis product of chlorophyll concentration as well as phytoplankton concentration, spatially averaged over the El-Nino 3.4 region; look up the raw data itself for units and the extend of time period (note that the El-Nino 3.4 SST data is monthly averaged, but spans Jan 1950 to Dec 2019).\n",
    "\n",
    "Do a similar analysis on this data, but also do some cross-correlations of the the three signals you now have (SST, chl-$a$ concentration and phytoplankton concentration) over the overlapping periods. Interpret the data accordingly, and see if you can do some physical rationalisations of the data. \n",
    "\n",
    "Consider doing some hypothesis testing too (e.g. El-Nino years cause a decrease in biological activity in the region); you may need to pull out or generate your own samples. Be careful with formulating your null hypothesis, choice of significance, interpretation of the conclusions, and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull or open files from the internet if needed (e.g. temporary session in Colab)\n",
    "# link = https://raw.githubusercontent.com/julianmak/OCES3301_data_analysis/main/elnino34_bgc.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Window functions\n",
    "\n",
    "[**Window functions**](https://en.wikipedia.org/wiki/Window_function) are really intended to be used to modify a signal to force it to be periodic, so that you can do Fourier analysis on it; see *08_time_series*. While they are not designed as kernels functions, there is no particularly reason you can't use them as kernel functions for filtering. Two exercises to try here:\n",
    "\n",
    "1) Choose a window (`signal.tukey` is as good a choice as any), look up the syntax and plot out the window function. What you then want to do is to normalise the kernel function so that the integral of the window function is 1 (hint: you probably want `np.trapz` somewhere to do the integral). Then, one option is to multiply the kernel function by the window size, multiply that scaled kernel function to a uniformly spaced signal, then compute the mean as in the filtering cases above to give you a value of the filtered function at some $t$. Because you multiply the intended kernel function by window size and then compute the mean of the product, you are preserving the magnitude of the function somewhat (if you miss one of these steps then the result will either be too large or too small). Try different choices of window sizes.\n",
    "\n",
    "2) Try other functions available in `scipy.signal` and see if there are any significant differences. Some of these windows have parameters associated with them (e.g. `signal.tukey` has an optional `alpha` parameter), look and see what differences those make too.\n",
    "\n",
    "It doesn't matter which signal you use on. Use the ones provides above or make your own up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) [Wavelets](https://en.wikipedia.org/wiki/Wavelet)\n",
    "\n",
    "**Wavelets** are a particular class of functions that can also serve as a basis, like the Fourier modes we are going to consider in *08_time_series*. You may or may not want to look up what wavelets are, and consider using the `pywt` package (which is external, so you might need to `conda` or `pip` it). One thing you could try here is to use wavelets as a way to filter noisy signals, as in the exercise above.\n",
    "\n",
    "If you want more info, one place I recommend looking into is Jonathan Lilly's [time-series course](https://github.com/jonathanlilly/time-series), which is much more advanced and much more thorough than the things I have here.\n",
    "\n",
    "> NOTE: To use Jonathan's material, either download the whole pack or `git clone` the pack, and open `index.html` in the downloaded folder in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
