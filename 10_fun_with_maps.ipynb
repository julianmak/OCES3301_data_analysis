{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdgT6FrBPqBq"
   },
   "source": [
    "*11 Jan 2025, Julian Mak (whatever with copyright, do what you want with this)\n",
    "\n",
    "### As part of material for OCES 3301 \"Data Analysis in Ocean Sciences\" delivered at HKUST\n",
    "\n",
    "For the latest version of the material, go to the public facing [GitHub](https://github.com/julianmak/academic-notes/tree/master/OCES3301_data_analysis_ocean) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22893,
     "status": "ok",
     "timestamp": 1745049782316,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "ntJwJM5qPqBr",
    "outputId": "fee7053b-a08d-4fa9-fc86-826e8ed1f54f"
   },
   "outputs": [],
   "source": [
    "# you probably need this to if you run things in Colab (it is a bit slow to download by the way...)\n",
    "#!pip install cartopy  # Colab give me version \"0.19.0.post1\" it would seem\n",
    "\n",
    "# load some deafult packages\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.xkcd() # xkcd default formatting; comment this if you don't want it\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "import copy\n",
    "from scipy import signal\n",
    "from scipy import interpolate\n",
    "from sklearn.decomposition import PCA  # not loading the StandardScaler here because it seems to screw it up...?\n",
    "\n",
    "# some modules for cartopy (I used 0.18.0 apparently, from \"cartopy.__version__\")\n",
    "# download cartopy not found\n",
    "try:\n",
    "  import cartopy\n",
    "except ModuleNotFoundError:\n",
    "  !pip install cartopy\n",
    "  import cartopy\n",
    "\n",
    "import cartopy.crs as ccrs  # crs = co-ordinate reference system, for the different map projections\n",
    "# some extra bits to modify the labels, lines and for making coluorbars\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# pull files from the internet if needed (e.g. temporary session in Colab)\n",
    "# !wget https://github.com/julianmak/OCES3301_data_analysis/raw/main/salinity_WOA13_decav_Reg1L46_clim.nc\n",
    "# !wget https://github.com/julianmak/OCES3301_data_analysis/raw/main/temperature_WOA13_decav_Reg1L46_clim.nc\n",
    "# !wget https://github.com/julianmak/OCES3301_data_analysis/raw/main/gebco_bathy_coarsen.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZI_pq3nGPqBs"
   },
   "source": [
    "---------------------------\n",
    "\n",
    "# 10: Fun with maps\n",
    "\n",
    "Lets try and draw some stuff on maps. While the data I've provided are in regular lon/lat grids so there is no strong need to use map projections, I do not provide masks or the like to plot land with, so it might be advantageous to have a package that can plot land features, coastlines and the like. You might want to focus on the poles, which the regular lon/lat projection will not be that sensible (because you are really trying to represent a disk as a cylinder, which involves a tear). There might be times when you want certain properties that simply cannot be represented in the regular lon/lat projection: for example, the area is not preserved, so giving rise to the problem that things at the equator look much smaller than they are than at the poles (e.g. Indian looks much smaller than Greenland, when actually the area is actually 1.5 times larger).\n",
    "\n",
    "Before we get to the `cartopy` package that does most of the heavy lifting for us, just a bit of (probably) useful mathematical background.\n",
    "\n",
    "> ## Key Objective(s)\n",
    "> 1. Introduce some aspects fo cartopgraphy.\n",
    "> 2. Use of the `cartopy` package for data transformation.\n",
    "> 3. Perform interpolation and/or extrapolation with multi-dimensional array data.\n",
    "> 4. Highlight links between Empirical Orthogonal Functions (EOFs) and PCA, and perform EOF analysis for spatial data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANSy8w5_PqBt"
   },
   "source": [
    "## Classical geometry (with some topology)\n",
    "\n",
    "Recall in `08_time_series` I floated the idea of a basis, where a vector can be represented in different ways (with the standard canonical basis $\\{\\boldsymbol{e}_1, \\boldsymbol{e}_2\\} = \\{(1, 0), (0, 1)\\}$, or some other way basis; see example in that notebook). Similarly, we can represent positions in different ways: $(1, 1)$ could be represented as the point $x=1$ and $y=1$, but I could also represent it as $r=\\sqrt{2}$ and $\\theta=\\pi/4$, where\n",
    "\n",
    "\\begin{equation*}\n",
    "    x = r\\cos\\theta, \\qquad y = r\\sin\\theta,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\theta$ is the angle I measure from the $x$-axis, and $r$ is the radius from the origin. This is essentially a change from **Cartesian co-ordinates** to **polar co-ordinates**, and the rule give about is a **co-ordinate transformation** from $(r,\\theta)$ to $(x,y)$. See drawing below.\n",
    "\n",
    "> NOTE: In fact it is customary to teach about co-ordinate transforms before bases.\n",
    "\n",
    "> NOTE: A co-ordinate transformation should strictly be reversible and one-to-one. The above fails at the isolated point $r=0$ corresponding to $(x,y)=(0,0)$, since I can choose any $\\theta$ I like there, so we have no uniqueness, but that can be manually patched out, so it can be regarded as a co-ordinate transformation.\n",
    "\n",
    "<img src=\"https://i.imgur.com/VFzpEO2.png\" width=\"400\" alt='cursed coordinate'>\n",
    "\n",
    "Again, there is nothing particularly mysterious about this, it's just representing things in a different (and perhaps more natural) way. For example, the function corresponding to the circle of radius one in Cartesian co-ordinates is (trust me on this, or try this yourselves with some coding)\n",
    "\n",
    "\\begin{equation*}\n",
    "    y = \\pm \\sqrt{1 - x^2}\n",
    "\\end{equation*}\n",
    "\n",
    "for $x$ running from $-1$ to $1$ (and you need the plus or minus to try the two segments separately). On the other hand, in polar co-ordinates this is simply\n",
    "\n",
    "\\begin{equation*}\n",
    "    r = 1\n",
    "\\end{equation*}\n",
    "\n",
    "for $\\theta$ running from $0$ to $2\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZVCfeavPqBu"
   },
   "source": [
    "The problem is particularly notable if you are talking about data on a sphere. If you are on the surface of the sphere, generically you will have to specify a ($x,y,z$), but if you take for example **spherical co-ordinates** ($r, \\theta, \\phi$), which corresponds here respectively to radius, longitude and latitude, then if we are on the surface of the Earth, then we set $r = R_{\\rm Earth}$, and we are only left with longitude and latitude, i.e. we need to carry one less variable around. If we only have two variables, then you can represent it as a 2d plot, which is of course easier (cf. `04_multilinear_regression`, where you use a PCA to get lower dimensional data).\n",
    "\n",
    "> NOTE: I'll leave you to look up the transformation rules between Cartesian and spherical co-ordinates. You will need this if you need to compute the **metric** terms to get **distances**, which is part of some extended exercises here and in the previous notebook.\n",
    "\n",
    "In principle you can plot data gridded at lon/lat as a flat 2d, but there is a fundamental limitation: the surface of the sphere (a 2-sphere or $S^2$) is fundamentally different to that of the plane, since the former is clearly curved while the other is flat. What this means is that the *topology* is fundamentally different (in this case through the Euler characteristic, which is related to the *Gaussian curvature* through the [Gauss-Bonnet theorem](https://en.wikipedia.org/wiki/Gauss%E2%80%93Bonnet_theorem); not going to elaborate on that). Topology is perhaps best explained through the classic example of a (cursed?) spherical cow and coffee cup / doughtnut:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/24/Spot_the_cow.gif?20211112013016\" width=\"200\" alt='cursed cow'><img src=\"https://c.tenor.com/W13M_b0FkVMAAAAd/topology-cup.gif\" width=\"200\" alt='coffee cup'>\n",
    "\n",
    "Here we can smoothly deform the cow into the sphere, and the cup into the doughnut, without \"tearing\" the surface. In that sense, they are topologically equivalent, and the cow is topologically distinct to the doughnut (the topological quantity here is the *genus*, which you could think of as \"holes\").\n",
    "\n",
    "Going back to the sphere case which is our main interest, since the sphere is topologically distinct to the plane, you have to introduce a \"tear\" to deform the sphere onto the plane (you actually only need to remove one point). The practical consequence here is that there is ***no way to preserve both areas and angles when doing planar map projections of spherical data***. You can mathematically prove this; there is no *isometry* (a distance preserving map) between the sphere and the Cartesian plane. What this means is any map projection you make you can:\n",
    "\n",
    "1) preserve angles (*conformal mapping*; e.g. Mercator, Lambert Conformal)\n",
    "\n",
    "2) preserve areas (*area-preserving*; e.g. Mollweide, Interrupted Goode Homolosine)\n",
    "\n",
    "3) neither of those (e.g. Plate Carree, Robinson, Orthographic)\n",
    "\n",
    "(An isometry does both 1 and 2.) The regular lon/lat projection is a case where you do neither. Sometimes you may or may not want some of these properties. There are multiple choices in how you choose the represent the lon/lat data on a flat space, given by various **map projections**, each with their own mathematical formula to translate from lon/lat to some ($x,y$) to give you the representation on the flat plane; see [here](https://en.wikipedia.org/wiki/Map_projection) for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WM-2zlPKPqBu"
   },
   "source": [
    "-------------------------------------\n",
    "## a) `cartopy`\n",
    "\n",
    "The spiel above is mostly driven by the fact I really wanted to include the spherical cow...anyway, the upshot here is that a lot of heavy lifting (particularly the maths) has actually been coded up and is already available in the package `cartopy` for example. I am going to demonstrate syntax with examples below; the way you might want to do this is to see what you like, and try and fish out the code that gives you the things you like. If there are other things you want to do, Google and Stack exchange/overflow is your friend (most of the time)...\n",
    "\n",
    "The bit of code below gives you a selection of map projections available.\n",
    "\n",
    "> NOTE: The below seems to give a lot of warnings now from Cartopy, but seems to be ignorable in that the intended outcome still shows as expected. Possibly to be fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 32150,
     "status": "ok",
     "timestamp": 1745049814472,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "jP9IQc_9PqBu",
    "outputId": "ccc0b225-73e5-4cb7-819c-6f9ac1392fa4"
   },
   "outputs": [],
   "source": [
    "# list of \"common\" projections\n",
    "# code adapted from Ryan Abernathey (https://rabernat.github.io/research_computing_2018/maps-with-cartopy.html)\n",
    "\n",
    "projections = [ccrs.PlateCarree(),\n",
    "               ccrs.Robinson(),\n",
    "               ccrs.Mercator(),\n",
    "               ccrs.Orthographic(),                                            # default center at (0,0)\n",
    "               ccrs.Orthographic(central_latitude=90),                         # northern orthographic\n",
    "               ccrs.Orthographic(central_longitude=90, central_latitude=-90),  # southern orthographic with shift\n",
    "               ccrs.Mollweide(),\n",
    "               ccrs.LambertConformal(),\n",
    "               ccrs.InterruptedGoodeHomolosine()\n",
    "              ]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 13))\n",
    "\n",
    "for i, proj in enumerate(projections):\n",
    "    ax = plt.subplot(3, 3, i+1, projection=proj)\n",
    "    ax.stock_img()\n",
    "    ax.coastlines()\n",
    "    ax.set_title(f\"{type(proj)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-h1n6ULPqBv"
   },
   "source": [
    "Lets try it on some data. Below code loads the WOA13 surface (conservative) temperature data in xarray, and then does a basic plot using the Plate Carree projection. The main addition here are:\n",
    "\n",
    "* the `projection=CARTOPY_PROJECTION` to the axes object, which makes it into a cartopy axes\n",
    "\n",
    "* `ax.add_feature(cartopy.feature.LAND)` which adds land features onto the map\n",
    "  * if you are running this in Colab, it will download the files on the fly, but if you exist Colab you will have to re-download it on the fly next time because none of the download on-the-fly packages/data are cached\n",
    "\n",
    "> NOTE: Instead of repeatedly called `ccrs.PlateCarree()` I just defined it as `pcarree = ccrs.PlateCarree()`. The reason is that the associated object is used in various places even if when we are not dealing with the Plate Carree projection itself. See examples later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1567,
     "status": "ok",
     "timestamp": 1745049816047,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "U1WkQeYAPqBv",
    "outputId": "3371601f-56ea-4a66-c8d5-7a67d88c622b"
   },
   "outputs": [],
   "source": [
    "# load the temperature profiles\n",
    "\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    file = \"temperature_WOA13_decav_Reg1L46_clim.nc\"\n",
    "elif option == \"remote\":\n",
    "    # do a local caching (downloads a file to cache)\n",
    "    print(\"loading data remotely\")\n",
    "    file_loc = \"https://github.com/julianmak/OCES3301_data_analysis/raw/refs/heads/main/temperature_WOA13_decav_Reg1L46_clim.nc\"\n",
    "    file = fsspec.open_local(f\"simplecache::{file_loc}\", filecache={'cache_storage': '/tmp/fsspec_cache'})\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "toce_WOA13 = xr.open_dataset(file)\n",
    "\n",
    "lon, lat = toce_WOA13[\"lon\"], toce_WOA13[\"lat\"]\n",
    "toce = toce_WOA13[\"votemper\"].isel(lev=0)  # only pick out the surface data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "executionInfo": {
     "elapsed": 3460,
     "status": "ok",
     "timestamp": 1745049819528,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "Z_LjYuVFPqBv",
    "outputId": "40a39d40-fd1d-48be-955f-1165138782ab"
   },
   "outputs": [],
   "source": [
    "# define the axes with projections\n",
    "pcarree = ccrs.PlateCarree()\n",
    "\n",
    "# define some plot options\n",
    "plot_opts = {\"cmap\"   : \"RdBu_r\",\n",
    "             \"levels\" : np.linspace(0, 30, 31),\n",
    "             \"extend\" : \"both\"}\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "kt = 0\n",
    "ax = plt.subplot(1, 2, 1, projection=pcarree)\n",
    "ax.contourf(lon, lat, toce.isel(time=kt), **plot_opts)\n",
    "ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "ax.set_title(f\"$\\Theta(z=0, t={toce.time[kt].values})$\")\n",
    "\n",
    "kt = 5\n",
    "ax = plt.subplot(1, 2, 2, projection=ccrs.PlateCarree())\n",
    "ax.contourf(lon, lat, toce.isel(time=kt), **plot_opts)\n",
    "ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "ax.set_title(f\"$\\Theta(z=0, t={toce.time[kt].values})$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3sVsqc7PqBw"
   },
   "source": [
    "The maps above could do with a bit more decoration, because for example it is not very obvious how different the January data (left) is to the June data (right). The decorations below are:\n",
    "\n",
    "* `gl=ax.gridlines` for gridlines\n",
    "  * modifications to grid lines through modifying the `gl` object attributes; see below\n",
    "\n",
    "* adding axes labels as `ax.text` (`ax.set_xlabel` behaves differently for the cartopy axes)\n",
    "\n",
    "* `divider = make_axes_locatable(ax)` in preparation for a colorbar (this is a native `matplotlib` command)\n",
    "  * done this way to make sure colorbar scales with the plot size, otherwise it looks a bit off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "executionInfo": {
     "elapsed": 4253,
     "status": "ok",
     "timestamp": 1745049823783,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "Zf5qGLOpPqBw",
    "outputId": "f0778786-aa9a-449d-ce7c-dcedf2715af2"
   },
   "outputs": [],
   "source": [
    "# as above but with a bit more decorations\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "kt = 0\n",
    "ax = plt.subplot(1, 2, 1, projection=pcarree)\n",
    "ax.contourf(lon, lat, toce.isel(time=kt), **plot_opts)\n",
    "ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "ax.set_title(f\"$\\Theta(z=0, t={toce.time[kt].values})$\")\n",
    "\n",
    "gl = ax.gridlines(crs=pcarree, draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "ax.set_extent([-180, 180, -80, 90], crs=pcarree)\n",
    "ax.text(0.5, -0.20, r'Lon $\\left( {}^\\circ \\right)$',\n",
    "        va='bottom', ha='center',\n",
    "        rotation='horizontal', rotation_mode='anchor',\n",
    "        transform=ax.transAxes)\n",
    "ax.text(-0.12, 0.5, r'Lat $\\left( {}^\\circ \\right)$',\n",
    "        va='bottom', ha='center',\n",
    "        rotation=90, rotation_mode='anchor',\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "kt = 5\n",
    "ax = plt.subplot(1, 2, 2, projection=pcarree)\n",
    "cs = ax.contourf(lon, lat, toce.isel(time=kt), **plot_opts)  # define cs for making the colorbar\n",
    "ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "ax.set_title(f\"$\\Theta(z=0, t={toce.time[kt].values})$\")\n",
    "\n",
    "gl = ax.gridlines(crs=pcarree, draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "gl.left_labels = False\n",
    "ax.set_extent([-180, 180, -80, 90], crs=pcarree)\n",
    "ax.text(0.5, -0.20, r'Lon $\\left( {}^\\circ \\right)$',\n",
    "        va='bottom', ha='center',\n",
    "        rotation='horizontal', rotation_mode='anchor',\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "divider = make_axes_locatable(ax)  # add a colorbar\n",
    "cb_ax = divider.append_axes(\"right\", size = \"2%\", pad = 0.2, axes_class=plt.Axes)\n",
    "cax = plt.colorbar(cs, cax=cb_ax)\n",
    "cax.set_ticks([0, 5, 10, 15, 20, 25, 30])\n",
    "cax.ax.set_title(r\"$^\\circ\\mathrm{C}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0p984ZeIPqBw"
   },
   "source": [
    "Below shows zooms and so forth, still sticking with the plate Carree projection. Some comments:\n",
    "\n",
    "* one could set the zoom by `ax.set_extent` if you are only dealing with `contourf`\n",
    "\n",
    "* when combined with contours I don't find the results very satisfactory, so what I did was to select the data I want through indexing (done via xarray), and then plot the data, instead of relying on `ax.set_extent`\n",
    "\n",
    "* I did the `contourf` results first and then overlaid the `lines=contour` results to show some labelled contour lines (`ax.clabel(lines, inline=True)`), using the same subsetting procedure as above\n",
    "  * if you rely on `ax.set_extent` then the labels may not show in your zoomed in region, and the inline labelling might look a bit weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "executionInfo": {
     "elapsed": 3868,
     "status": "ok",
     "timestamp": 1745049827652,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "f3bepCD_PqBw",
    "outputId": "c8ec0c59-bf8f-480d-dca1-40a1d580c47c"
   },
   "outputs": [],
   "source": [
    "# zoom in to various places\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "kt = 0\n",
    "\n",
    "plot_opts = {\"cmap\"   : \"RdBu_r\",\n",
    "             \"levels\" : np.linspace(0, 30, 31),\n",
    "             \"extend\" : \"both\"}\n",
    "\n",
    "# define limits to do things over: North Atlantic here\n",
    "lon_lim, lat_lim = (-80, 10), (0, 65)\n",
    "\n",
    "ax = plt.subplot(1, 2, 1, projection=pcarree)\n",
    "\n",
    "# note: the contour labels and gridlines are nicer if you force a plot based on the sliced out data\n",
    "#       (another way to do it is to set the extent, see the commented line below)\n",
    "ax.contourf(lon.sel(lon=slice(lon_lim[0], lon_lim[1])),\n",
    "            lat.sel(lat=slice(lat_lim[0], lat_lim[1])),\n",
    "            toce.isel(time=kt).sel(lon=slice(lon_lim[0], lon_lim[1]), lat=slice(lat_lim[0], lat_lim[1])),\n",
    "            **plot_opts)\n",
    "lines = ax.contour(lon.sel(lon=slice(lon_lim[0], lon_lim[1])),\n",
    "                   lat.sel(lat=slice(lat_lim[0], lat_lim[1])),\n",
    "                   toce.isel(time=kt).sel(lon=slice(lon_lim[0], lon_lim[1]), lat=slice(lat_lim[0], lat_lim[1])),\n",
    "                   colors=\"k\", levels=np.arange(5, 31, 5))\n",
    "ax.clabel(lines, inline=True)\n",
    "ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "ax.set_title(f\"$\\Theta(z=0, t={toce.time[kt].values})$\")\n",
    "\n",
    "gl = ax.gridlines(crs=pcarree, draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "# ax.set_extent([lon_lim[0], lon_lim[1], lat_lim[0], lat_lim[1]], crs=pcarree)\n",
    "ax.text(0.5, -0.20, r'Lon $\\left( {}^\\circ \\right)$',\n",
    "        va='bottom', ha='center',\n",
    "        rotation='horizontal', rotation_mode='anchor',\n",
    "        transform=ax.transAxes)\n",
    "ax.text(-0.12, 0.5, r'Lat $\\left( {}^\\circ \\right)$',\n",
    "        va='bottom', ha='center',\n",
    "        rotation=90, rotation_mode='anchor',\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "# define limits to do things over: South China Sea + Pacific\n",
    "lon_lim, lat_lim = (100, 140), (0, 45)\n",
    "\n",
    "plot_opts = {\"cmap\"   : \"RdBu_r\",\n",
    "             \"levels\" : np.linspace(10, 30, 30),\n",
    "             \"extend\" : \"both\"}\n",
    "\n",
    "ax = plt.subplot(1, 2, 2, projection=pcarree)\n",
    "cs = ax.contourf(lon.sel(lon=slice(lon_lim[0], lon_lim[1])),\n",
    "                 lat.sel(lat=slice(lat_lim[0], lat_lim[1])),\n",
    "                 toce.isel(time=kt).sel(lon=slice(lon_lim[0], lon_lim[1]), lat=slice(lat_lim[0], lat_lim[1])),\n",
    "                 **plot_opts)  # define cs for making the colorbar\n",
    "lines = ax.contour(lon.sel(lon=slice(lon_lim[0], lon_lim[1])),\n",
    "                   lat.sel(lat=slice(lat_lim[0], lat_lim[1])),\n",
    "                   toce.isel(time=kt).sel(lon=slice(lon_lim[0], lon_lim[1]), lat=slice(lat_lim[0], lat_lim[1])),\n",
    "                   colors=\"k\", levels=np.arange(10, 30, 2))\n",
    "ax.clabel(lines, inline=True)\n",
    "ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "ax.set_title(f\"$\\Theta(z=0, t={toce.time[kt].values})$\")\n",
    "\n",
    "gl = ax.gridlines(crs=pcarree, draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "# ax.set_extent([lon_lim[0], lon_lim[1], lat_lim[0], lat_lim[1]], crs=pcarree)\n",
    "ax.text(0.5, -0.20, r'Lon $\\left( {}^\\circ \\right)$',\n",
    "        va='bottom', ha='center',\n",
    "        rotation='horizontal', rotation_mode='anchor',\n",
    "        transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVgIEtQPPqBx"
   },
   "source": [
    "In the below I do something similar but for the GEBCO bathymetry data and showing these in different map projections (the Mercator and North Pole centered orthographic). Comments:\n",
    "\n",
    "* the different projections are achieved by changing the `projection` keyword in the axes object\n",
    "\n",
    "* note the addition in this case of `transform=pcarree` in the `contourf` commands\n",
    "  * no I don't know why the transform is pcarree when the projection is Mercator, if you find out tell me...\n",
    "  \n",
    "* I downsized the data because any transform away from standard pcarree can take some time (I think this is especially true when the orthographic projections are concerned)\n",
    "  * downsizing by a factor of 4 (`[::4]`) seems to be alright, but you will notice it still takes a bit of time to show the results\n",
    "  * downsizing by anything larger than 6 I found the results started to look a bit too smudged and losing visually noticeable details, but feel free to experiment with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1745049828170,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "UAW0POQPPqBx",
    "outputId": "21948c67-2b93-4793-caac-476c4d6da391"
   },
   "outputs": [],
   "source": [
    "# load bathymetry data\n",
    "\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    file = \"gebco_bathy_coarsen.nc\"\n",
    "elif option == \"remote\":\n",
    "    # do a local caching (downloads a file to cache)\n",
    "    print(\"loading data remotely\")\n",
    "    file_loc = \"https://github.com/julianmak/OCES3301_data_analysis/raw/refs/heads/main/gebco_bathy_coarsen.nc\"\n",
    "    file = fsspec.open_local(f\"simplecache::{file_loc}\", filecache={'cache_storage': '/tmp/fsspec_cache'})\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "bathy = xr.open_dataset(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 62160,
     "status": "ok",
     "timestamp": 1745049890345,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "_CP5LrcAPqBx",
    "outputId": "f23d9388-7635-4228-e4a5-15a1eb4d139d"
   },
   "outputs": [],
   "source": [
    "# plotting bathymetry but in a different projection\n",
    "\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "plot_opts = {\"cmap\"   : \"gist_earth\",\n",
    "             \"levels\" : np.linspace(-5000, 5000, 30),\n",
    "             \"extend\" : \"both\"}\n",
    "\n",
    "# mercator projection\n",
    "ax = plt.subplot(1, 2, 1, projection=ccrs.Mercator())\n",
    "ax.contourf(bathy[\"lon\"][::4],\n",
    "            bathy[\"lat\"][::4],\n",
    "            bathy[\"elev\"][::4, ::4],\n",
    "            transform=pcarree, **plot_opts)  #\n",
    "ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "ax.set_title(f\"elevation from GEBCO\")\n",
    "\n",
    "gl = ax.gridlines(crs=pcarree, draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "ax.set_extent([-180, 180, -80, 90])\n",
    "ax.text(0.5, -0.20, r'Lon $\\left( {}^\\circ \\right)$',\n",
    "        va='bottom', ha='center',\n",
    "        rotation='horizontal', rotation_mode='anchor',\n",
    "        transform=ax.transAxes)\n",
    "ax.text(-0.12, 0.5, r'Lat $\\left( {}^\\circ \\right)$',\n",
    "        va='bottom', ha='center',\n",
    "        rotation=90, rotation_mode='anchor',\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "# northern orthographic (sometimes northern stereographic)\n",
    "ax = plt.subplot(1, 2, 2, projection=ccrs.Orthographic(central_longitude=0, central_latitude=90))\n",
    "cs = ax.contourf(bathy[\"lon\"][::4],\n",
    "                 bathy[\"lat\"][::4],\n",
    "                 bathy[\"elev\"][::4, ::4],\n",
    "                 transform=pcarree, **plot_opts)  # define cs for making the colorbar\n",
    "ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "ax.set_title(f\"elevation from GEBCO\")\n",
    "ax.set_global()  # NOTE: seem to need this magic line to display the plot in this projection\n",
    "\n",
    "gl = ax.gridlines(crs=pcarree, draw_labels=True,\n",
    "                  linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "\n",
    "divider = make_axes_locatable(ax)  # add a colorbar\n",
    "cb_ax = divider.append_axes(\"right\", size = \"2%\", pad = 0.2, axes_class=plt.Axes)\n",
    "cax = plt.colorbar(cs, cax=cb_ax)\n",
    "cax.ax.set_title(r\"$\\mathrm{m}$\")\n",
    "cax.set_ticks([-5000, -2500, 0, 2500, 5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NhS0tfxPqBx"
   },
   "source": [
    "> <span style=\"color:red\">**Q.**</span> Have some fun with these, maybe make some movies with the data or whatever. Ask me if you want some data like `current_speed.nc` below but over a different region to generate movies with (e.g. data just off the Drake passage in the Southern Ocean for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwG84jTCPqBx"
   },
   "source": [
    "--------------------------\n",
    "# b) Spatial interpolation\n",
    "\n",
    "Recall that in *08_time_series* we had missing data and we ended up interpolating in time. We can basically do the same thing in space. The ideas and things to be aware of are basically the same, except there are subtleties with extra dimensions you have to be careful about (e.g. a cubic spline is a polynomial fitting of data points, but what does a 2d polynomial even mean?)\n",
    "\n",
    "Will just go straight into an example of this that also combines the use of Cartopy above. Below demonstration is an outline on how I generated parts of the figure in a recent paper ([Thomy et al 2024](https://doi.org/10.1093/ismeco/ycae109), Fig 1a), which is to plot a reconstruction/modelled Sea Surface Salinity over the HK area. The complications and the wish to do interpolation will arise as I show the data.\n",
    "\n",
    "First we load the salinity data. The data is originally from [HYCOM](https://www.hycom.org/), and I took the surface salinity over the month of June in some year and then averaged it in time, so the data is only a function of latitude and longitude, at about 9km horizontal resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "executionInfo": {
     "elapsed": 721,
     "status": "ok",
     "timestamp": 1745049891075,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "be-brBdjPqBx",
    "outputId": "f3c54b84-f416-46af-a8f9-e04b450d97ab"
   },
   "outputs": [],
   "source": [
    "# load bathymetry data\n",
    "\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    file = \"June_2020_meanSalinity.nc\"\n",
    "elif option == \"remote\":\n",
    "    # do a local caching (downloads a file to cache)\n",
    "    print(\"loading data remotely\")\n",
    "    file_loc = \"https://github.com/julianmak/OCES3301_data_analysis/raw/refs/heads/main/June_2020_meanSalinity.nc\"\n",
    "    file = fsspec.open_local(f\"simplecache::{file_loc}\", filecache={'cache_storage': '/tmp/fsspec_cache'})\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "hycom_data = xr.open_dataset(file)\n",
    "hycom_data = hycom_data.isel(depth=0) # get rid of that floating singleton dimension\n",
    "hycom_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1745049891334,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "AqdgNE5fPqBx",
    "outputId": "8d4b248f-21e2-44ac-b093-89fc87e2b591"
   },
   "outputs": [],
   "source": [
    "# raw plot\n",
    "hycom_data[\"salinity\"].plot(vmin = 20, vmax = 36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2sZjjSXPqBy"
   },
   "source": [
    "If you squint your eyes a bit, the modelled Pearl River Delta region is the bit with lower salinity (because of freshwater input). The salinity signal indicates the flow out of the river goes towards the southwest, which is consistent with the Summer phase of the monsoon over the region (cf. `lec02`, `lec05`, `lec09` and `lec10` of `OCES2003` lecture). HK and the surrounding islands are those little white dots to the east of the Pearl River.\n",
    "\n",
    "The above is using `pcolor` that plots per grid cell. We could try and see if plotting with `contourf` would be a bit better. Going to enable some of the `cartopy` features also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "executionInfo": {
     "elapsed": 9777,
     "status": "ok",
     "timestamp": 1745049901114,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "GhuF8gyVPqBy",
    "outputId": "fb220787-9c85-4904-876b-fa315e915186"
   },
   "outputs": [],
   "source": [
    "# plotting raw hycom data using contourf\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = plt.axes(projection = ccrs.PlateCarree())\n",
    "ax.set_extent([113.2, 114.7, 21.8, 23.0],ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                  linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "ax.add_feature(cartopy.feature.LAND, zorder=3, edgecolor = 'k')\n",
    "\n",
    "ax.contourf(hycom_data[\"lon\"], hycom_data[\"lat\"], hycom_data[\"salinity\"],\n",
    "            levels=np.linspace(20, 36, 31), extend=\"both\", cmap = \"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vD6B064PqBy"
   },
   "source": [
    "Looks a bit ugly doesn't it? The white blocks are where there is no data (`NaN`s), so at least `contourf` did the nice thing by ignoring those, otherwise the code would fail. What we can do is to fill out some of the white blocks via interpolation, and then try again.\n",
    "\n",
    "Recall the previous for interpolation we had to provide co-ordinates and the values, i.e.\n",
    "* an 1d array `t` where there is data\n",
    "* an 1d array `data` (or `f` in the previous notebooks) where the data is\n",
    "and we had to blank out the `NaN` values, otherwise the interpolator breaks. Doing it in 2d is similar, and we need to provide (in this case)\n",
    "* a 1d x 2 array `coords` where the data lives, where the array consists of a collection of locations\n",
    "* a 1d array `data` (or `f` in the previous notebooks) where the data lives\n",
    "This part is slightly different because by default the data is in the form\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{pmatrix}\n",
    "        f(y_1, x_1) & f(y_2, x_1) & \\ldots & f(y_n, x_1)\\\\\n",
    "        f(y_1, x_2) & \\ddots      & \\cdots & \\vdots\\\\\n",
    "        \\vdots      & \\ddots      & \\ddots & \\vdots\\\\\n",
    "        f(y_1, x_m) & \\cdots      & \\cdots & f(y_n, x_m)\n",
    "    \\end{pmatrix}, \\qquad\n",
    "    \\begin{pmatrix}\n",
    "        y_1 & y_2 & \\ldots & y_n\\\\\n",
    "        y_1 & \\ddots      & \\cdots & \\vdots\\\\\n",
    "        \\vdots      & \\ddots      & \\ddots & \\vdots\\\\\n",
    "        y_1 & \\cdots      & \\cdots $ y_n\n",
    "    \\end{pmatrix}, \\qquad\n",
    "    \\begin{pmatrix}\n",
    "        x_1 & x_1 & \\ldots & x_1\\\\\n",
    "        x_2 & \\ddots      & \\cdots & \\vdots\\\\\n",
    "        \\vdots      & \\ddots      & \\ddots & \\vdots\\\\\n",
    "        x_m & \\cdots      & \\cdots $ x_m\n",
    "    \\end{pmatrix},\n",
    "\\end{equation*}\n",
    "\n",
    "or similar for the data and co-ordinate arrays. What we really want to do is for them to be written as\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{pmatrix}\n",
    "        f(y_1, x_1) \\\\\n",
    "        f(y_2, x_1) \\\\\n",
    "        \\vdots \\\\\n",
    "        f(y_n, x_1)\\\\\n",
    "        f(y_1, x_2)\\\\\n",
    "        \\vdots\\\\\n",
    "        f(y_n, x_m)\n",
    "    \\end{pmatrix}, \\qquad\n",
    "    \\begin{pmatrix}\n",
    "        y_1 & x_1 \\\\\n",
    "        y_2 & x_1 \\\\\n",
    "        \\vdots & \\vdots \\\\\n",
    "        y_n & x_1\\\\\n",
    "        y_1 & x_2\\\\\n",
    "        \\vdots & \\vdots\\\\\n",
    "        y_n & x_m\n",
    "    \\end{pmatrix},\n",
    "\\end{equation*}\n",
    "i.e. as a \"flattened\" or \"reshaped\" into a collection of data and co-ordinates. The below code does this as a whole block:\n",
    "\n",
    "0) pull out the relevant data as an `numpy` array (don't really have to do this, but doing it for safety)\n",
    "1) find out the indices of all the `NaN` points first (which are land points)\n",
    "2) the co-ordinate files are 1d, but going to bulk it out to 2d corresponding to the data points\n",
    "3) pick out only the data that are not `NaN` (i.e. the wet points)\n",
    "4) reshape co-ordinate and data arrays from 2d meshes onto 1d-like arrays (done here using the `.flatten()` command tagged with numpy arrays; the bulking out above makes sure it is flattened consistently)\n",
    "5) throw this into a choice of 2d interpolator (just chose one here)\n",
    "6) evaluate accordingly even over the land points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1745049901124,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "NkbHVC64PqBy"
   },
   "outputs": [],
   "source": [
    "# interpolating hycom data\n",
    "\n",
    "# 0) pull out data\n",
    "sal_hycom = hycom_data[\"salinity\"].values\n",
    "lon_hycom = hycom_data[\"lon\"].values\n",
    "lat_hycom = hycom_data[\"lat\"].values\n",
    "\n",
    "# 1) pick out land points (done as a boolean)\n",
    "nan_inds = np.isnan(sal_hycom)\n",
    "\n",
    "# 2) bulk out the co-ordinates\n",
    "xx, yy = np.meshgrid(lon_hycom, lat_hycom)\n",
    "orig_shape = xx.shape # for reverting to original shape later\n",
    "\n",
    "# 3) pick out all the *non*-NaN points\n",
    "wet_vals = np.zeros((np.sum(~nan_inds), 2))  # \"sum\" finds how many wet points there are\n",
    "wet_vals[:, 0] = xx[~nan_inds] # this already flattens the array\n",
    "wet_vals[:, 1] = yy[~nan_inds]\n",
    "\n",
    "# 4, 5) flatten array and throw into interpolator (Clougher2D is like a cubic spline fitting)\n",
    "# the [~nan_inds] already flattens array\n",
    "f = interpolate.CloughTocher2DInterpolator(wet_vals, sal_hycom[~nan_inds])\n",
    "\n",
    "# 6) re-evaluate at all points\n",
    "lonlat = np.zeros((xx.size, 2))\n",
    "lonlat[:, 0] = xx.flatten()  # this one does need flattening\n",
    "lonlat[:, 1] = yy.flatten()\n",
    "sal_interp = f(lonlat).reshape(orig_shape)  # evalute on flattened array, then revert it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1745049901483,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "lb0iJq9ZPqBy",
    "outputId": "461788e7-d12f-46ed-ba21-4945f2f020a2"
   },
   "outputs": [],
   "source": [
    "# plot out the data with pcolor to make a point\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.pcolor(lon_hycom, lat_hycom, sal_hycom, vmin=20, vmax=36, cmap = \"viridis\")\n",
    "ax.set_title(\"original data\")\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.pcolor(lon_hycom, lat_hycom, sal_interp, vmin=20, vmax=36, cmap = \"viridis\")\n",
    "ax.set_title(\"interpolated data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eM_a9u3PqBy"
   },
   "source": [
    "Notice the interpolator provides some numbers at some points but not others (returning `NaN` at those places).\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Can you see/guess the condition on where the interpolator provides results? (hint: the definition of ***convex hull*** might help)\n",
    "\n",
    "Some interpolated values are probably a bit dubious (e.g. the ones on the west side of the Pearl River Delta), and the values that are now over land never existed in the first place so should not be used really. For the purposes of the interpolation was done to provide a reasonable plot: the dubious values get covered up by the land anyway, and we never do calculations with it, so it's at least justified from that point of view. Covering it with the land and tidying it results in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "executionInfo": {
     "elapsed": 923,
     "status": "ok",
     "timestamp": 1745049902408,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "6BLr33WEPqBy",
    "outputId": "c9d36341-61c7-4b47-f41e-a91f39c66c57"
   },
   "outputs": [],
   "source": [
    "# plotting interpolated hycom data step\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = plt.axes(projection = pcarree)\n",
    "ax.set_extent([113.75, 114.5, 22.05, 22.6])  # zoom in over HK\n",
    "ax.coastlines()\n",
    "\n",
    "gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                  linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "ax.add_feature(cartopy.feature.LAND, zorder=3, edgecolor = 'k')\n",
    "\n",
    "cs = ax.contourf(lon_hycom, lat_hycom, sal_interp, levels=np.linspace(20, 36, 31),\n",
    "                 extend=\"both\", cmap = \"viridis\")\n",
    "\n",
    "# create the axes by grabbing the bounding boxes of the individual plot axes\n",
    "divider = make_axes_locatable(ax)\n",
    "ax_cb = divider.new_horizontal(size=\"2%\", pad=0.1, axes_class=plt.Axes)\n",
    "\n",
    "# create the axes by grabbing the bounding boxes of the individual plot axes\n",
    "divider = make_axes_locatable(ax)\n",
    "ax_cb = divider.new_horizontal(size=\"2%\", pad=0.1, axes_class=plt.Axes)\n",
    "\n",
    "fig.add_axes(ax_cb)\n",
    "plt.colorbar(cs, cax=ax_cb)\n",
    "ax_cb.set_ylabel(r\"Salinity\")\n",
    "ax_cb.set_title(r\"$\\mathrm{g}/\\mathrm{kg}$\")\n",
    "\n",
    "cs.set_edgecolors(\"face\") # old magic command to get rid of white lines in contourf outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vz3KOuV2PqBz"
   },
   "source": [
    "> <span style=\"color:red\">**Q.**</span> Look up the manual to see what other interpolators are available, and try to swap out interpolators to see if differences and similarities (particularly try `scipy.interpolate.interp2d(kind=\"linear\")` or `nearest`).\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Try computing statistics and the like and see how badly it gets screwed up by the presence of the interpolators.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Some locations are definitely bad for computing statistics on (the clearly land points), but some might be defensible (e.g. some of the land points according to HYCOM, but are really blocked out because of islands). Have a think and try and identify how to pick out the partially wet poitns, and compute statistics over those instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "241pooypPqBz"
   },
   "source": [
    "--------------------------\n",
    "# c) Empirical Orthogonal Functions (EOFs)\n",
    "\n",
    "#### ***TL;DR: basically PCAs***\n",
    "\n",
    "When we were dealing with Fourier series we essentially assumed we have a basis function in advance (see `08_time_series` and above) and expand our function with it, and as a result we deal with the amplitude. This is all well and good if you know your basis in advance, but sometimes that doesn't work (e.g. complicated domain shapes), and sometimes you don't necessarily want to do that. We may for example seek something like\n",
    "\n",
    "\\begin{equation*}\n",
    "    f(t,x,y) = \\sum_{k=1}^N \\mbox{PC}_k(t)\\ \\mbox{EOF}_k(x,y)\n",
    "\\end{equation*}\n",
    "\n",
    "where the function $\\mbox{EOF}(x,y)$ is some spatial pattern that captures the most variability in the data in some sense, and tagged on with it some $\\mbox{PC}(t)$ that describes that pattern's evolution in time. The idea then is that the first few EOFs would be related to the large-scale persistent (in space and time) features, and other things would be additional details on top of that. We might then be more inclined to try and come up with theories that explain the dominant EOFs, or constrain theories based on whether they are able to reproduce these EOFs.\n",
    "\n",
    "Sounds familiar? Well that's basically what the PCA (`04_regression`) aims to do as well. The EOF analysis could be thought of as a generalised PCA, and while there are some detailed differences, the principles and the algorithms involved are more similar than they are different.\n",
    "\n",
    "The main technical difference is that, for the PCA, we essentially want to form the **covariance matrix** (see `04_regression`), which is necessarily symmetric by construction, diagnonalise the matrix, from which the eigenvectors are the PCs, and the eigenvalues are the variance explained. For the EOF, what in generally do is to start with $f(t,x,y)$, reshape it into $f(t, \\mathrm{space})$, and generically the dimensions of $t$ and $\\mathrm{space}$ will not agree, and if we form the covariance matrix we would have to artificially bulk out locations with empty numbers to make sure the eventual covariance matrix is square. To avoid this, we generally consider what would be called a **singular value decomposition (SVD)**, which considers the decomposition\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbf{\\mathsf{F}} = \\mathbf{\\mathsf{U}} \\mathbf{\\mathsf{\\Sigma}} \\mathbf{\\mathsf{V}}^T,\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\mathbf{\\mathsf{F}}$ is the matrix encoding $f(t, \\mathrm{space})$\n",
    "* $\\mathbf{\\mathsf{\\Sigma}}$ is the matrix containing the **singular values** $d\\sim \\sigma^2$, where $\\sigma^2$ is the variance\n",
    "* $\\mathbf{\\mathsf{U}}$ is the matrix of the **principal components**, where the *rows* correspond to $\\mbox{PC}_k(t)$ (e.g. `U[:, 0]` would be $\\mbox{PC}_1(t)$)\n",
    "* $\\mathbf{\\mathsf{V}}^T$ is the matrix containing the **empirical orthogonal functions**, where *columns* of $\\mathbf{\\mathsf{V}}^T$ correspond to $\\mbox{EOF}_k(\\mathrm{space})$ (e.g. `VT[0, :]` would be $\\mbox{EOF}_0(\\mathrm{space})$, which needs to be reshaped back to $\\mbox{EOF}_0(x, y)$)\n",
    "\n",
    "The ordering is such that we are in descending order of the magnitude of the singular values. Put another way, the PCs and EOFs are really the **left/right singular vectors** ($\\mathbf{\\mathsf{U}}$ and $\\mathbf{\\mathsf{V}}$ are the matrices containing the singular vectors). The maximum number of singular values (i.e. maximum size of $\\mathbf{\\mathsf{\\Sigma}}$) and thus the maximum number of singular vectors is the minimum of the size between $t$ and $\\mathrm{space}$.\n",
    "\n",
    "> NOTE: Most PCA algorithms actually use the SVD approach, instead of the eigenvalue approach I described. SVDs can be applied to non-square matrices, and the algorithms are generally stable and fairly fast.\n",
    "\n",
    "Unless you are really keen we don't actually need to implement the EOF analysis by hand from scratch (unless you want to, it's not actually that hard; see optional exercise at the end). We can leverage `PCA` in the `scikit-learn` package as is, and the only thing we need to do is do some pre-processing of data before feeding data into the PCA, and undo the processing afterwards for the EOFs (the PCs drops out as is)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIKScMz1PqBz"
   },
   "source": [
    "## Example: \"trivial\" sines and cosines\n",
    "\n",
    "Let's try it with a simple example first. The below code is copy and pasted from `09_fun_with_maps` for the purposes of demonstrating how to make animations. The function we define here is\n",
    "\n",
    "\\begin{equation*}\n",
    "    f(t, y, x) = \\sin(x)\\cos(y)\\sin(t) + 0.5 \\cos(2x) \\cos(2t).\n",
    "\\end{equation*}\n",
    "\n",
    "The first part on the right hand side is from `09_fun_with_maps`, and I added on the the second part for demonstration reasons. Given your now extensive experience with Fourier analysis (!), you would know that a Fourier analysis of $f(t,y,x)$ would unequivocally pick out the two contributions separately. The question here is what would the EOF analysis do?\n",
    "\n",
    "Intuitively, we would expect the EOF analysis to also pick our those components. Given the choice of amplitudes I fed into defining $f$, we might expect that\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mbox{EOF}_1(x,y) \\sim \\sin(x)\\cos(y), \\qquad \\mbox{PC}_1(t) \\sim \\sin(t),\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mbox{EOF}_2(x,y) \\sim \\cos(2x), \\qquad \\mbox{PC}_2(t) \\sim \\cos(2t),\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mbox{EOF}_k(x,y) \\sim 0, \\qquad \\mbox{PC}_k(t) \\sim 0, \\qquad k \\geq 3\n",
    "\\end{equation*}\n",
    "\n",
    "where I used the squiggle $\\sim$ to denote that these things would be defined up to a constant factor (negative or positive). Additionally, we should be able to reconstruct exactly (or at least numerically) the function $f$ from the EOFs and PCs.\n",
    "\n",
    "Let's define the raw data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1745049902416,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "ulG5MB68PqBz"
   },
   "outputs": [],
   "source": [
    "# snapshots with a different colour map to show oscillations\n",
    "\n",
    "x_vec = np.linspace(0, 2*np.pi, 21)\n",
    "y_vec = np.linspace(0, 2*np.pi, 26)\n",
    "t_vec = np.linspace(0, 2*np.pi, 21)\n",
    "\n",
    "xx, yy = np.meshgrid(x_vec, y_vec)  # creates a mesh (a 2d array) from the 1d arrays\n",
    "f = np.zeros((len(t_vec), len(y_vec), len(x_vec)))  # use (t, y, x) ordering\n",
    "for kt in range(len(t_vec)):\n",
    "    t = t_vec[kt]\n",
    "    f[kt, :, :] =  (        np.sin(  xx) * np.cos(yy) * np.sin(t)\n",
    "                    + 0.5 * np.cos(2*xx)              * np.cos(2*t)\n",
    "                   )\n",
    "del xx, yy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0IOpFtxPqBz"
   },
   "source": [
    "Recall that from the PCA we want to scale the data and get some anomalies (done by a $Z$-transform, through the `StandardScaler` in `sklearn`). For the EOF we want to scale the input data too, but it looks like in this case we want to de-trend or at least de-mean in time per spatial point, rather than do a $Z$-transform.\n",
    "\n",
    "The below carries out a de-trending (linear trend) using `scipy.signal` only over the time dimension; the data here is arranged as $(t,y,x)$ so it is `axis=0` that we want.\n",
    "\n",
    "> NOTE: If we de-mean, then you want to add the mean back on right at the end when (and if) you are trying to reconstruct the original function from the EOFs and PCs. If we de-trend, then it looks like you basically don't need to do anything more. I can't get it working for the $Z$-transform at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1745049902422,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "zRb66GBZPqBz"
   },
   "outputs": [],
   "source": [
    "# probably want to at least de-mean in time\n",
    "#   if de-mean, take mean off, get EOFs, but add mean back on to final result)\n",
    "# f_mean = np.mean(f, axis=0)\n",
    "\n",
    "# detrend (to get anomalies with respect to linear trend in time) seems to work fine\n",
    "#    with some minor points in the PCs\n",
    "f = signal.detrend(f, axis=0)  # could also use keyword \"overwrite_data=True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O_eLtJcPqBz"
   },
   "source": [
    "The next bit is to reshape the matrix from $f(t, y, x)$ into $f(t, \\mathrm{space})$, so from a 3d to a 2d array. This is done via the `np.reshape` command. We will be passing the 2d array to the PCA in `scikit-learn`, and at some point we will want to invert the (2d) EOFs back into 3d.\n",
    "\n",
    "> NOTE: If you look these things up online you might see that some of the Python implementations have the optional keyword `order=\"F\"`, which means Fortran ordering (which differs from the default which is `order=\"C\"`, which is C ordering). I have not found that it made any difference as long as you are consistent when reshaping and un-reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1745049902430,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "tTXD0A-WPqBz"
   },
   "outputs": [],
   "source": [
    "# reshape from (t, y, x) into (t, space),\n",
    "# NOTE: order='F' being Fortran ordering might be seen sometimes\n",
    "#       don't think this matters as long as you are consistent in the reshaping\n",
    "\n",
    "X = np.reshape(f, (len(t_vec), len(y_vec) * len(x_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lkg12akaPqB0"
   },
   "source": [
    "Now we pass the 2d array to PCA routine, which we already encountered in `04_regression`, so I am just going to state it. The PCs in this case is the transformation of the input data, while the EOFs are the components. Below I print out the explained variance ratio (it is a variance presumably converted from the calculated singular values), as well reshape the $\\mbox{EOF}_k(\\mathrm{space})$, which is a 2d array (labelled here by $k$ and $\\mathrm{space}$), back into $\\mbox{EOF}_k(y, x)$, which is a 3d array (labelled here by $k$, $y$ and $x$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1745049902464,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "45kKiZswPqB0",
    "outputId": "733cf454-e367-49bb-9901-0eee44d75aa4"
   },
   "outputs": [],
   "source": [
    "# basically do a PCA\n",
    "pca = PCA(n_components=5)\n",
    "\n",
    "# pull out the PCs and EOFs\n",
    "PCs             = pca.fit_transform(X)\n",
    "EOFs_to_reshape = pca.components_\n",
    "\n",
    "print(f\"pca var explained = {pca.explained_variance_ratio_ * 100} %\")\n",
    "\n",
    "# reshape the EOFs from (EOF number, space) to  (EOF number, y, x)\n",
    "EOFs = np.reshape(EOFs_to_reshape, (EOFs_to_reshape.shape[0], len(y_vec), len(x_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26S3CYlnPqB0"
   },
   "source": [
    "From the variance explained, we can see that basically all the variance is captured by the first two EOFs, which is exactly what we expected. We will plot out the first two EOFs (which is a function of space) and their associated PCs (which is a function of time) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 720
    },
    "executionInfo": {
     "elapsed": 865,
     "status": "ok",
     "timestamp": 1745049903322,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "4xZGQwZ8PqB9",
    "outputId": "04d00976-e52b-4cf6-8404-ab14e3216b0b"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "cs = ax.contourf(x_vec, y_vec, EOFs[0, :, :], cmap=\"RdBu_r\", levels=16)\n",
    "ax.set_title(r\"$\\mathrm{EOF}_1(x,y)$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "plt.colorbar(cs)\n",
    "\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "ax.plot(t_vec, PCs[:, 0])\n",
    "ax.set_title(r\"$\\mathrm{PC}_1(t)$\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "cs = ax.contourf(x_vec, y_vec, EOFs[1, :, :], cmap=\"RdBu_r\", levels=16)\n",
    "ax.set_title(r\"$\\mathrm{EOF}_2(x,y)$\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "plt.colorbar(cs)\n",
    "\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "plt.plot(t_vec, PCs[:, 1])\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_title(r\"$\\mathrm{PC}_2(t)$\")\n",
    "ax.grid(lw=0.5, zorder=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14dgo0PSPqB9"
   },
   "source": [
    "So, interestingly, while the PCs corresponding to the EOFs are what we expected (e.g. the $\\cos(2x)$ patterns with the $\\cos(2t)$ time series), the EOF ordering is the other way round. It turns out this is somewhat of an artifact of the choice of pre-processing: if you do a de-mean procedure instead then the EOFs are the \"right\" way round. Notice here PC2 looks a bit weird, in that it's not exactly a $\\sin(t)$, which is what we imposed when defining $f(t, y, x)$.\n",
    "\n",
    "In that regard, this is a warning that the EOF analysis requires some sort of anomalies, and can be sensitive to the choice of pre-processing.\n",
    "\n",
    "The below test utilises the EOFs and PCs to reconstruct the data via\n",
    "\n",
    "\\begin{equation*}\n",
    "    f(t,x,y) = \\sum_{k=1}^2 \\mbox{PC}_k(t)\\ \\mbox{EOF}_k(x,y)\n",
    "\\end{equation*}\n",
    "\n",
    "The plot is per time $\\hat{t}$, and shows the individual $\\mbox{PC}_k(\\hat{t})\\ \\mbox{EOF}_k(x,y)$, the full data $f(\\hat{t}, y, x)$, and the signed mismatch between $f(\\hat{t}, y, x)$ and $\\sum_{k=1}^2 \\mbox{PC}_k(\\hat{t})\\ \\mbox{EOF}_k(x,y)$. The signed mismatch can be shown to be shown to be tiny and essentially at the machine level for all the time levels.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Don't take my word for it, try it and convince yourself I didn't just outright lie in the above two claims (the dependence on pre-processing and the smallness of the signed mismatch).\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Ideally by code rather than examining the plots visually, demonstrate that numerically the mismatches are small (hint: consider examining the absolute value of the mismatch at every single time step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "executionInfo": {
     "elapsed": 1077,
     "status": "ok",
     "timestamp": 1745049904400,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "iDi_QGJEPqB9",
    "outputId": "5f127732-1b2a-4c21-9a21-d1c340096bec"
   },
   "outputs": [],
   "source": [
    "# reconstruct the data at some time\n",
    "\n",
    "t_ind = 15  # field constructued here to not be identically zero at any choice of t\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "cs = ax.contourf(x_vec, y_vec, EOFs[0, :, :] * PCs[t_ind, 0], cmap=\"RdBu_r\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.set_title(f\"$\\mathrm{{EOF}}_1(x,y) \\mathrm{{PC}}_1(t={t_vec[t_ind]/(2.0*np.pi):.2f}$ wavelength$)$\")\n",
    "plt.colorbar(cs)\n",
    "\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "cs = ax.contourf(x_vec, y_vec, EOFs[1, :, :] * PCs[t_ind, 1], cmap=\"RdBu_r\")\n",
    "ax.set_title(f\"$\\mathrm{{EOF}}_2(x,y) \\mathrm{{PC}}_2(t={t_vec[t_ind]/(2.0*np.pi):.2f}$ wavelength$)$\")\n",
    "plt.colorbar(cs)\n",
    "\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "cs = ax.contourf(x_vec, y_vec, f[t_ind, :, :], cmap=\"RdBu_r\")\n",
    "ax.set_title(f\"$f(x,y, t={t_vec[t_ind]/(2.0*np.pi):.2f}$ wavelength$)$\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "plt.colorbar(cs)\n",
    "\n",
    "# reconstruct the data from the EOFs\n",
    "f_EOFs = np.zeros((len(y_vec), len(x_vec)))\n",
    "for i in range(2):\n",
    "    f_EOFs += EOFs[i, :, :] * PCs[t_ind, i]\n",
    "\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "cs = ax.contourf(x_vec, y_vec, f[t_ind, :, :] - f_EOFs, cmap=\"RdBu_r\") # detrend case\n",
    "ax.set_title(r\"$f - f_{\\rm EOFs}$\")\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "plt.colorbar(cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P_iQUK2PqB9"
   },
   "source": [
    "## Example: ERA SST reanalysis data\n",
    "\n",
    "Lets try this for \"real\" (ocean) data, partly to demonstrate extra things that we might need to be careful with the analysis, in this case when we encounter land points.\n",
    "\n",
    "Below code uses xarray to load the [extended reconstructed SST anomaly data](https://www.eea.europa.eu/data-and-maps/data/external/noaa-extended-reconstructed-sea-surface), and just prints out the metadata.\n",
    "\n",
    "> NOTE: We are going to deal with the anomalies `ssta` directly. There is the full `sst` data on the repository too, but I have to say I have not managed to reconstruct the anomalies from the full data. Partly because I am not entirely sure what the anomalies are relative to (it says relative to some climatology, but that didn't seem to work for me)...\n",
    ">\n",
    "> If I were to redo this I would get rid of the linear trend in time from the full data to remove the global warming signal. You can try this yourself, here or in `assignment 4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "executionInfo": {
     "elapsed": 2586,
     "status": "ok",
     "timestamp": 1745049906987,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "4hdN-06zPqB9",
    "outputId": "25e12e7d-a73b-4c2a-a291-5df06b7f4b04"
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/julianmak/OCES3301_data_analysis/raw/main/ersstv5_ssta.nc  # processed anomaly data\n",
    "# !wget https://github.com/julianmak/OCES3301_data_analysis/raw/main/ersstv5_sst.nc   # full data\n",
    "\n",
    "# load the already processed anomalies\n",
    "\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    file = \"ersstv5_ssta.nc\"\n",
    "elif option == \"remote\":\n",
    "    # do a local caching (downloads a file to cache)\n",
    "    print(\"loading data remotely\")\n",
    "    file_loc = \"https://github.com/julianmak/OCES3301_data_analysis/raw/refs/heads/main/ersstv5_ssta.nc\"\n",
    "    file = fsspec.open_local(f\"simplecache::{file_loc}\", filecache={'cache_storage': '/tmp/fsspec_cache'})\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = xr.open_dataset(file)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fRNKOuBPqB9"
   },
   "source": [
    "Note that the data is monthly. I am going to analyse the Atlantic (the Pacific would be for the assignment or as an extended exercise later). This is partly to demonstrate another issue: data arrangement.\n",
    "\n",
    "Note that the longitude from metadata goes from 0 to 360. While the data technically is to be interpreted as periodic in longitude, because of how it is set up the data is centered over the Pacific, while if we want the Atlantic (in this case between -80 and 10), xarray would complain in the selection, either giving you longitudes from 0 to 10, or from 280 to 360 (which is -80 to 0). A simple fix here is to just shift the co-ordinate, which is done with the `assign_coords` for an xarray dataset (the `% 360` is *modulo 360*, where the input numbers gets a multiple of 360 taken off to put the input numbers between 0 and 359). We are then going to do the selection using the `sel` command, which comes with xarray objects (saves us having to look up the corresponding indices).\n",
    "\n",
    "From the metadata, we also see that there is a hanging `lev` dimension floating around (it's just of length 1). Going to get rid of that below by doing `isel(lev=0)`. Going to follow the zeroth rule of data analysis and just plot a sample of the data to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1745049907239,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "ixKj9lRXPqB-",
    "outputId": "0a02a8b3-9b82-4547-f260-576a887b98ec"
   },
   "outputs": [],
   "source": [
    "# selection over Atlantic\n",
    "# current longitude is (0, 360), focus over Pacific\n",
    "# want Atlantic, quickest thing is to shift the longitude (-180, 180) so centered over the Atlantic\n",
    "df = df.assign_coords(lon=(((df[\"lon\"] + 180) % 360) - 180)).sortby('lon')\n",
    "\n",
    "target_lon, target_lat = slice(-80, 10), slice(0, 90)\n",
    "\n",
    "# trivial selection to drop the z co-ordinate (there is only 1)\n",
    "ssta = df[\"ssta\"].isel(lev=0).sel(lon=target_lon, lat=target_lat)\n",
    "\n",
    "# pull out other useful things to carry around\n",
    "time = df[\"time\"].values\n",
    "lon  = df[\"lon\"].sel(lon=target_lon).values\n",
    "lat  = df[\"lat\"].sel(lat=target_lat).values\n",
    "ssta.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JI45RMbePqB-"
   },
   "source": [
    "Here the white entries in the plot are the **land points**, which in the array is indicated by a `NaN` (you can look this up yourself by examining the data matrix itself). What you will find is that PCA can't seem to handle `NaN` entries, so we have to do something about it. There are two ways to do this:\n",
    "\n",
    "1) use masked arrays in numpy `numpy.ma` probably\n",
    "\n",
    "2) only do the PCA over the ocean points (I am going to call these **wet points**)\n",
    "\n",
    "I am going to do the second procedure as it is more robust and general (although somewhat more involved); you can try the first one as a coding excercise.\n",
    "\n",
    "First I am going to reshape the array into $(t, \\mathrm{space})$ as above. Then, since we know the land points correspond to `NaN` entries, we pick out the indices that are *not* `NaNs`, done through:\n",
    "\n",
    "* `np.where(CONDITION)` which picks out the indices corresponding the conditions\n",
    "  * `np.where` returns a tuple containing the indices, so there is a `[0]` to pick out the indices\n",
    "* `np.isnan(ARRAY)` to identify `NaN` entries with `True`\n",
    "  * `~` is `NOT`, so `~np.isnan(ARRAY)` is to pick out the entries that are *not* `NaN`s\n",
    "  * we assume the land points don't change in time, so we only need to query the 1st time entry, hence `ARRAY[0, :]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 83,
     "status": "ok",
     "timestamp": 1745049907321,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "dlMgYk0NPqB-"
   },
   "outputs": [],
   "source": [
    "# already have anomalies, so could just go straight ahead with the EOF analysis\n",
    "#   not going to weight the gridcells here by the area\n",
    "\n",
    "X = np.reshape(ssta.values, (len(time), len(lat) * len(lon)))\n",
    "wet_pts = np.where(~np.isnan(X[0, :]))[0]  # pick out indices corresponding to ocean pts at first time index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fK6khaaNPqB-"
   },
   "source": [
    "We now throw this in to the PCA algorithm, keeping quite a few components in this case. The below output shows a bar graph indicating the percentage of variance explained per EOF (as the blue bars), and the *cumulative* percentage of variance explained by the first $k$ number of EOFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "executionInfo": {
     "elapsed": 709,
     "status": "ok",
     "timestamp": 1745049908032,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "TwFXv-klPqB-",
    "outputId": "16790903-8bac-42db-c29f-092db28fce6b"
   },
   "outputs": [],
   "source": [
    "# basically do a PCA:\n",
    "pca = PCA(n_components=20)\n",
    "\n",
    "# pull out the PCs and EOFs\n",
    "PCs             = pca.fit_transform(X[:, wet_pts])  # only do EOF analysis over the wet points\n",
    "EOFs_to_reshape = pca.components_\n",
    "\n",
    "print(f\"pca var explained = {pca.explained_variance_ratio_}\")\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = plt.axes()\n",
    "ax.bar(np.arange(0, 20), pca.explained_variance_ratio_, color=\"C0\",\n",
    "       label=r\"EOF var explained\",\n",
    "       zorder=3)\n",
    "ax.bar(np.arange(0, 20), np.cumsum(pca.explained_variance_ratio_), color=\"C1\",\n",
    "       label=\"total EOF explained\",\n",
    "       zorder=2, alpha=0.7)  # push to the back\n",
    "ax.legend()\n",
    "ax.grid(lw=0.5, zorder=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnPmSCHKPqB-"
   },
   "source": [
    "Notice here unlike the artifical example above, the EOFs don't explain that much variance, so we might want to keep a few more if we wanted to reconstruct the original signal (you can do this as an exercise).\n",
    "\n",
    "The code below then reshapes the EOFs back to real space, but here we need to deal with the land points. So what I do below is:\n",
    "\n",
    "* set up a ($\\mathrm{EOF}, \\mathrm{space}$) array where everything are `NaN`s\n",
    "* per EOF, overwrite the corresponding wet points according to the `wet_pts` array with the EOFs\n",
    "* once all the EOFs have been dumped out, reshape the EOFs to ($\\mathrm{EOF}, y, x$) array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1745049908036,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "_mq8PoAMPqB-"
   },
   "outputs": [],
   "source": [
    "# reshape the EOFs from (EOF number, space) to  (EOF number, y, x)\n",
    "\n",
    "# set a load of land points as (EOF, space)\n",
    "EOFs = np.ones((EOFs_to_reshape.shape[0], len(lat) * len(lon))) * np.nan\n",
    "\n",
    "# overwrite the places where it is actually ocean, per EOF\n",
    "for k in range(EOFs.shape[0]):\n",
    "    EOFs[k, wet_pts] = EOFs_to_reshape[k, :]\n",
    "\n",
    "# reshape the EOFs back to (EOF, lat, lon)\n",
    "EOFs = np.reshape(EOFs, (EOFs.shape[0], len(lat), len(lon)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-4fQIEgPqB-"
   },
   "source": [
    "In the below I plot out the first four EOFs and the corresponding PCs. Explanation in the cell after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "executionInfo": {
     "elapsed": 2922,
     "status": "ok",
     "timestamp": 1745049910959,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "hosZIHYlPqB_",
    "outputId": "5e65cd80-d567-448c-b033-fdbbe7fe68a5"
   },
   "outputs": [],
   "source": [
    "# plot a few EOFs out\n",
    "pcarree = ccrs.PlateCarree()\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "for k in range(4):\n",
    "\n",
    "    # pull out the plots and set the\n",
    "    EOF = EOFs[k, :, :]\n",
    "    limit = np.nanmax(np.nanmax(np.abs(EOF)))\n",
    "    levels = np.linspace(-limit, limit, 16)\n",
    "\n",
    "    ax = plt.subplot(2, 4, 2*k+1, projection=pcarree)\n",
    "    cs = ax.contourf(lon, lat, EOF, cmap=\"RdBu_r\", levels=levels)\n",
    "    gl = ax.gridlines(crs=pcarree, draw_labels=True,\n",
    "                      linewidth=2, color='gray', alpha=0.5, linestyle='--')\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    ax.add_feature(cartopy.feature.LAND, zorder = 10, edgecolor = 'k')\n",
    "    ax.set_title(f\"$\\mathrm{{EOF}}_{k+1}$\")\n",
    "\n",
    "    divider = make_axes_locatable(ax)  # add a colorbar\n",
    "    cb_ax = divider.append_axes(\"right\", size = \"2%\", pad = 0.2, axes_class=plt.Axes)\n",
    "    cax = plt.colorbar(cs, cax=cb_ax)\n",
    "    cax.set_ticks([-limit, 0, limit])\n",
    "\n",
    "    ax = plt.subplot(2, 4, 2*k+2)\n",
    "    ax.plot(time, PCs[:, k], color=\"C0\")\n",
    "    ax.set_xlabel(f\"$t$\")\n",
    "    ax.set_ylabel(f\"$\\mathrm{{PC}}_{k+1}$\")\n",
    "    plt.setp(plt.gca().xaxis.get_majorticklabels(),\n",
    "        'rotation', 90)\n",
    "    ax.grid(lw=0.5, zorder=0)\n",
    "\n",
    "fig.tight_layout(pad=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7zpPUERPqB_"
   },
   "source": [
    "With regards to interpretations, I am going to speculate a bit here, and I am willing to be shown wrong in my interpretations.\n",
    "\n",
    "### EOF 1\n",
    "\n",
    "I would have initially thought this would correspond to the global warming signal (partly given that the PCs are increasing over time) since I am almost certain that the anomalies are not relative to the linear trend over the whole period so that the global warming signal has not been removed, but it probably also includes the [**Atlantic Multi-decadal Variability (AMV)**](https://en.wikipedia.org/wiki/Atlantic_multidecadal_oscillation), which is a long-time scale variability on the order of 40 or so years probably (a quick look at the power spectrum suggests there is a peak at 60 years).\n",
    "\n",
    "### EOF 2\n",
    "\n",
    "This one is probably related to the [**North Atlantic Oscillation**](https://en.wikipedia.org/wiki/North_Atlantic_oscillation), with the sandwich pattern with the filling just south of Greenland, sandwiched between the blobs Gulf Stream region and North East of Iceland (very technical language here).\n",
    "\n",
    "### EOF 3\n",
    "\n",
    "Apart from this looking like the EOF 2 in Figure 3 of [Buckley et al. (2014)](https://www.researchgate.net/publication/277677765_Low-Frequency_SST_and_Upper-Ocean_Heat_Content_Variability_in_the_North_Atlantic/figures?lo=1), I don't have much more to add...\n",
    "\n",
    "### EOF 4\n",
    "\n",
    "No idea. This one seems to have a long period, so may also be related somewhat to AMV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54gzn4bvPqB_"
   },
   "source": [
    "> NOTE: This stuff is probably also highlighting that there are a few things one might need to do as pre/post-processing to get more information. These would include:\n",
    ">\n",
    "> 1) taking the global warming trend out, which would modify the EOF analysis probably (pre-processing)\n",
    ">\n",
    "> 2) might want to also take the seasonal cycle out via averaging over a few months (pre/post-processing)\n",
    ">\n",
    "> 3) window the PCs a bit to make the time-series analysis a bit clearer (post-processing)\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Do a time-series analysis on the PCs, picking out periods and the like.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Try doing some of the things I suggested above.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (coding) I am sure there are better ways to write the code that I did above, particularly with making better use of the xarray functionalities. Try and improve on my code (let me know if you do if you are happy to share).\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (coding) Try replacing my indexing procedure to get wet points with using the masked array functionality.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> (exploratory) Try different spatial regions. The equatorial Pacific is a particularly interesting region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-5sMjhBPqB_"
   },
   "source": [
    "----------------\n",
    "# More involved exercises with this notebook\n",
    "\n",
    "## 1) Spectral analysis for GEBCO bathymetry data\n",
    "\n",
    "Consider trying the Fourier analyses things on GEBCO data. Some things to bear in mind and think about:\n",
    "\n",
    "1) why might it not be valid to do a Fourier analysis with the global GEBCO data?\n",
    "\n",
    "2) if you do an analysis over a large region, you may want to adjust your conversion from lon/lat degrees to (kilo)meters from the one I used, because the assumption of 1 degree = 100 km then is even more dodgy, since the conversion is a function of latitude\n",
    "\n",
    "3) consider doing the analyses in patches, and see if there is some robustness in the power spectrum (e.g. [Goff & Jordan, 1989](https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/GL016i001p00045))\n",
    "\n",
    "<img src=\"https://i.imgur.com/bvclGhT.jpg\" width=\"600\" alt='picture of patches'>\n",
    "\n",
    "(a picture of the (in)famous Patches)\n",
    "\n",
    "4) consider doing the ocean and land parts separately\n",
    "\n",
    "5) [pretty involved] the proper thing to do is probably something like a **spherical harmonic analysis**, but you will probably need to go beyond native `numpy` and `scipy` packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1745049910963,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "1bXALrY_PqB_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzb8pm0ePqB_"
   },
   "source": [
    "## 2) [Probably quite involved] Spilhaus projection\n",
    "\n",
    "Work out how you might make the [Spilhaus projection](https://storymaps.arcgis.com/stories/756bcae18d304a1eac140f19f4d5cb3d) in Python:\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Michael-Meredith-5/publication/334126444/figure/fig2/AS:775590957309952@1561926702371/The-globe-viewed-on-a-Spilhaus-projection-in-contrast-to-conventional-projections-this_W640.jpg\" width=\"400\" alt='spilhaus'>\n",
    "\n",
    "(Cursed projection? Figure from Mike Meredith.)\n",
    "\n",
    "There is some work being done on the Cartopy repository itself with sporadic activity. If you get this working, I would suggest you make a pull request for Cartopy, because that would certainly be an addition I'd like to see in Cartopy (I think the above is made is a GIS software)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1745049910966,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "XHp19aeyPqB_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WDikd7APqB_"
   },
   "source": [
    "## c) Analysis of `current_speed.nc`\n",
    "\n",
    "Try the EOF analysis for the `current_speed.nc` data and see if anything interesting comes out. There should be no land points. Try also doing some time series analysis on the resulting PCs (e.g. Fourier analysis, suitably averaged etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1745049910969,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "kJNhQzEiPqCA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_78hHjTPqCA"
   },
   "source": [
    "## d) EOFs by hand\n",
    "\n",
    "Try doing EOFs by hand. The command you probably want is `np.linalg.svd(X, full_matrices=False)` (the last optional keyword means you don't save the singular values as a diagonal matrix).\n",
    "\n",
    "You can also try using EOFs for time-series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1745049910971,
     "user": {
      "displayName": "Jonathan Lee",
      "userId": "00111392245489590530"
     },
     "user_tz": -480
    },
    "id": "H9O7ezi4PqCA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
